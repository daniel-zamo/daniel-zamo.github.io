# Contenido de Archivos del Proyecto

Fecha de generaci√≥n: vie 09 ene 2026 10:24:41 CET

## Contenido archivo: `src/content.config.ts`

```bash
$ cat src/content.config.ts
import { defineCollection, z } from 'astro:content';
import { docsSchema } from '@astrojs/starlight/schema';

// 1. Colecci√≥n 'docs' (Tu documentaci√≥n principal)
const docsCollection = defineCollection({
  schema: docsSchema({
    extend: z.object({
      // Mapeo de campos personalizados
      source: z.string().optional(),

      // Taxonom√≠a de dominios
      domain: z.enum([
        'compute',
        'storage',
        'networking',
        'security',
        'virtualization',
        'containers',
        'automation',
        'monitoring',
        'cloud'
      ]).optional(),

      // Tipo de contenido (A√±adido 'project' para portfolio)
      type: z.enum([
        'lab',
        'theory',
        'cheatsheet',
        'scenario',
        'configuration',
        'project'
      ]).default('theory'),
    }),
  })
});

// 2. Colecci√≥n 'questions' (Para futuros quizzes)
const questionsCollection = defineCollection({
  loader: async () => [], // Placeholder: Astro 5 pide un loader si no hay archivos f√≠sicos aun
  schema: z.object({
    exam: z.string(),
    topic: z.string(),
    difficulty: z.enum(['easy', 'medium', 'hard']),
    question: z.object({
      scenario: z.string().optional(),
      prompt: z.string(),
    }),
    options: z.array(
      z.object({
        id: z.string(),
        text: z.string(),
      })
    ),
    correctAnswerId: z.string(),
    explanation: z.object({
      summary: z.string(),
      breakdown: z.array(
        z.object({
          optionId: z.string(),
          reasoning: z.string(),
        })
      ),
    }),
  }),
});

// 3. Exportamos
export const collections = {
  docs: docsCollection,
  // Si a√∫n no tienes archivos .json/.yaml en src/content/questions, 
  // puedes comentar la siguiente l√≠nea para evitar warnings de "colecci√≥n vac√≠a"
  questions: questionsCollection,
};


```

---

## Contenido archivo: `src/styles/custom.css`

```bash
$ cat src/styles/custom.css
/* ============================================
   ESTILOS M√çNIMOS - Solo dropdowns del header
   ============================================ */

/* Fondo oscuro para los select del header */
/*
header starlight-theme-select select,
header starlight-lang-select select {
  background-color: var(--sl-color-gray-6);
  color: var(--sl-color-white);
  border: 1px solid var(--sl-color-gray-5); 
} 
*/

/* Hover en los select */
/*
header starlight-theme-select select:hover,
header starlight-lang-select select:hover {
  background-color: var(--sl-color-gray-5);
  border-color: var(--sl-color-gray-4);
}
*/
header starlight-theme-select select:hover,
header starlight-lang-select select:hover {
  background-color: var(--sl-color-gray-5);
}

/* Opciones del dropdown (para Firefox) */
/*
header starlight-theme-select select option,
header starlight-lang-select select option {
  background-color: var(--sl-color-gray-6);
  color: var(--sl-color-white);
}
*/

/* Modo Light */
/*
[data-theme="light"] header starlight-theme-select select,
[data-theme="light"] header starlight-lang-select select {
  background-color: var(--sl-color-white);
  color: var(--sl-color-gray-6);
  border-color: var(--sl-color-gray-4);
}
*/

/*
[data-theme="light"] header starlight-theme-select select:hover,
[data-theme="light"] header starlight-lang-select select:hover {
  background-color: var(--sl-color-gray-2);
}
*/

/*
[data-theme="light"] header starlight-theme-select select option,
[data-theme="light"] header starlight-lang-select select option {
  background-color: var(--sl-color-white);
  color: var(--sl-color-gray-6);
}
*/
```

---

## Contenido archivo: `src/components/Quiz.astro`

```bash
$ cat src/components/Quiz.astro
---
// RUTA: src/components/Quiz.astro (VERSI√ìN FINAL CON MEJORAS)
import type { CollectionEntry } from 'astro:content';

interface Props {
  question: CollectionEntry<'questions'>['data'];
  questionId: string;
  prevId: string | null;
  nextId: string | null;
}
const { question, questionId, prevId, nextId } = Astro.props;
const formId = `quiz-form-${questionId}`;
---

<div class="quiz-container">
  {question.question.scenario && <p class="scenario"><em>{question.question.scenario}</em></p>}
  <p class="prompt">{question.question.prompt}</p>

  <form 
    class="options-form" 
    id={formId}
    data-correct-answer-id={question.correctAnswerId}
    data-explanation={JSON.stringify(question.explanation)}
  >
    {question.options.map(option => (
      <div class="option">
        <input type="radio" id={`option-${option.id}-${questionId}`} name={`quiz_option_${questionId}`} value={option.id}>
        <label for={`option-${option.id}-${questionId}`}>{option.text}</label>
      </div>
    ))}

    <div class="button-group">
      <button type="submit">Check Answer</button>
      <button type="reset" class="reset-button">Clear</button>
      
      <div class="navigation-buttons">
        {prevId && <a href={`/practice/quiz?id=${prevId}`} class="nav-button"><< Prev</a>}
        {nextId && <a href={`/practice/quiz?id=${nextId}`} class="nav-button">Next >></a>}
      </div>
    </div>
  </form>

  <div class="feedback-container" style="display: none;">
    <h3 class="feedback-title"></h3>
    <p class="feedback-summary"></p>
    <h4>Breakdown:</h4>
    <ul class="feedback-breakdown"></ul>
  </div>
</div>

<style>
  .quiz-container { border: 1px solid #ccc; padding: 1.5rem; border-radius: 8px; margin-top: 2rem; background-color: var(--sl-color-gray-6); }
  .scenario { border-left: 3px solid var(--sl-color-accent); padding-left: 1rem; color: var(--sl-color-gray-3); }
  .prompt { font-size: 1.2em; font-weight: bold; }
  .options-form .option { margin: 0.8rem 0; }
  .options-form button { margin-top: 1rem; padding: 0.5rem 1rem; border-radius: 4px; border: none; background-color: var(--sl-color-accent); color: white; cursor: pointer; }
  .feedback-container { margin-top: 1.5rem; padding-top: 1.5rem; border-top: 1px solid #ccc; }
  .button-group { display: flex; align-items: center; gap: 1rem; margin-top: 1.5rem; flex-wrap: wrap; }
  .reset-button { background-color: var(--sl-color-gray-4); color: var(--sl-color-text); border: none; padding: 0.5rem 1rem; border-radius: 4px; cursor: pointer; }
  .navigation-buttons { margin-left: auto; display: flex; gap: 0.5rem; }
  .nav-button { display: inline-block; padding: 0.5rem 1rem; border: 1px solid var(--sl-color-gray-4); border-radius: 4px; text-decoration: none; color: var(--sl-color-text); transition: background-color 0.2s; }
  .nav-button:hover { background-color: var(--sl-color-gray-5); }
  
/* .nav-button { display: inline-block; padding: 0.5rem 1rem; border: 1px solid var(--sl-color-gray-4); border-radius: 4px; text-decoration: none; color: var(--sl-color-text); transition: background-color 0.2s; } */
</style>

```

---

## Contenido archivo: `src/components/LabEnvironment.astro`

```bash
$ cat src/components/LabEnvironment.astro
---
/* src/components/LabEnvironment.astro */
import { Image } from "astro:assets";
// Aseg√∫rate de que tu imagen est√© en src/assets/images/
import topologyImg from "../assets/images/lab-01-topology.png"; 
---

<div class="lab-card">
  <!-- Cabecera -->
  <div class="card-header">
    <span class="icon">üñß</span>
    <strong>Infrastructure Context</strong>
  </div>

  <!-- Imagen del Diagrama -->
  <div class="img-wrapper">
    <Image
      src={topologyImg}
      alt="Network Topology Diagram"
      width={800}
      class="topology-img"
    />
  </div>

  <!-- Contenido -->
  <div class="info-content">
    <p class="intro-text">
      Access via SSH from the <strong>default terminal</strong>. The cluster
      topology is defined below:
    </p>
    
    <!-- Simulaci√≥n de archivo /etc/hosts -->
    <div class="terminal-window">
      <div class="terminal-bar">
        <span class="dot red"></span>
        <span class="dot yellow"></span>
        <span class="dot green"></span>
        <span class="terminal-title">üìÑ /etc/hosts (Cluster Inventory)</span>
      </div>
      <div class="terminal-body">
        <pre><code>
<span class="comment"># Management & Controller</span>
10.0.0.15   terminal       <span class="comment"># Main Entry Point</span>

<span class="comment"># Web & Application Tier</span>
10.0.0.60   web-srv1   <span class="comment"># Nginx / Apache</span>
10.0.0.70   app-srv1    <span class="comment"># Backend App</span>

<span class="comment"># Data Persistence Tier</span>
10.0.0.80   data-001    <span class="comment"># Primary DB</span>
10.0.0.90   data-002    <span class="comment"># Replica / Backup</span>

<span class="comment"># Virtualization & Containers</span>
10.0.0.100  kvm-001        <span class="comment"># Libvirt Hypervisor</span>
10.0.0.110  docker-001     <span class="comment"># Container Runtime</span>
</code></pre>
      </div>
    </div>

    <!-- Alerta de Seguridad -->
    <div class="security-alert">
      <div class="alert-icon">‚ö†Ô∏è</div>
      <div class="alert-text">
        <strong>Constraint:</strong> Nested SSH is forbidden. Connect directly from
        <code>terminal</code> to target IPs.
      </div>
    </div>
  </div>
</div>

<style>
  /* 1. Contenedor Principal */
  .lab-card {
    border: 1px solid var(--sl-color-gray-5);
    background-color: var(--sl-color-gray-6);
    border-radius: 0.5rem;
    overflow: hidden;
    margin-bottom: 2rem;
    box-shadow: var(--sl-shadow-sm);
  }

  /* 2. Cabecera */
  .card-header {
    background-color: var(--sl-color-gray-5);
    padding: 0.5rem 1rem;
    font-size: 0.9rem;
    border-bottom: 1px solid var(--sl-color-gray-4);
    display: flex;
    align-items: center;
    gap: 0.5rem;
    font-weight: 600;
    color: var(--sl-color-white);
  }

  /* 3. Imagen */
  .img-wrapper {
    /* Usamos variable de fondo para adaptarse si alg√∫n d√≠a pones modo claro */
    background-color: var(--sl-color-black); 
    padding: 1rem;
    display: flex;
    justify-content: center;
    border-bottom: 1px solid var(--sl-color-gray-5);
  }

  .topology-img {
    height: auto;
    max-width: 100%;
    object-fit: contain;
    border-radius: 4px;
  }

  /* 4. Contenido */
  .info-content {
    padding: 1.2rem;
  }

  .intro-text {
    margin-top: 0;
    margin-bottom: 1rem;
    font-size: 0.9rem;
    color: var(--sl-color-gray-2);
  }

  /* 5. Terminal Window */
  .terminal-window {
    background-color: #1e1e1e; /* Mantenemos oscuro duro para look terminal */
    border-radius: 6px;
    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);
    margin-bottom: 1.5rem;
    border: 1px solid #333;
    overflow: hidden;
  }

  .terminal-bar {
    background-color: #2d2d2d;
    padding: 6px 10px;
    display: flex;
    align-items: center;
    border-bottom: 1px solid #333;
  }

  .dot {
    height: 10px;
    width: 10px;
    border-radius: 50%;
    display: inline-block;
    margin-right: 6px;
  }
  .dot.red { background-color: #ff5f56; }
  .dot.yellow { background-color: #ffbd2e; }
  .dot.green { background-color: #27c93f; }

  .terminal-title {
    margin-left: 10px;
    color: #999;
    font-family: monospace;
    font-size: 0.8rem;
  }

  .terminal-body {
    padding: 1rem;
    overflow-x: auto;
  }

  pre {
    margin: 0;
    font-family: var(--sl-font-mono); /* Usar fuente mono del tema */
    font-size: 0.85rem;
    line-height: 1.5;
    color: #e0e0e0;
    background: transparent !important;
    border: none !important;
  }

  .comment {
    color: #6a9955;
    font-style: italic;
  }

  /* 6. Alerta - Usando colores sem√°nticos de Starlight */
  .security-alert {
    display: flex;
    align-items: center;
    gap: 0.8rem;
    /* Usamos variables de advertencia de Starlight */
    background-color: var(--sl-color-orange-low); 
    border: 1px solid var(--sl-color-orange-high);
    padding: 0.75rem;
    border-radius: 6px;
    color: var(--sl-color-orange-high);
    font-size: 0.85rem;
  }

  .alert-icon {
    font-size: 1.2rem;
  }
</style>


```

---

## Contenido archivo: `src/components/QuestionList.astro`

```bash
$ cat src/components/QuestionList.astro
---
// RUTA: src/components/QuestionList.astro
import { getCollection } from 'astro:content';
const allQuestions = await getCollection('questions');
---

<ul>
  {allQuestions.map(question => (
    <li>
      <a href={`/practice/quiz?id=${question.id}`}>
        Question about: {question.data.topic} ({question.data.difficulty})
      </a>
    </li>
  ))}
</ul>

<style>
  li {
    margin-bottom: 0.5rem;
  }
</style>

```

---

## Contenido archivo: `src/components/ReferenceArchitecture.astro`

```bash
$ cat src/components/ReferenceArchitecture.astro
---
/* 
   COMPONENT: ReferenceArchitecture.astro
   REPLACES: LabEnvironment.astro
   PURPOSE: Muestra la topolog√≠a l√≥gica del entorno usando Mermaid en lugar de im√°genes est√°ticas.
*/
---

<div class="arch-card">
  <div class="card-header">
    <span class="icon">üèóÔ∏è</span>
    <strong>Standard Operating Environment (SOE)</strong>
  </div>

  <div class="diagram-wrapper">
    <pre
      class="mermaid">
      graph TD
        %% Definici√≥n de Estilos
        classDef mgmt fill:#263238,stroke:#eceff1,color:#fff;
        classDef compute fill:#0277bd,stroke:#b3e5fc,color:#fff;
        classDef data fill:#2e7d32,stroke:#c8e6c9,color:#fff;
        classDef virt fill:#ef6c00,stroke:#ffe0b2,color:#fff;

        subgraph Management_Zone [Zone: Management]
            Bastion[("üíª terminal<br />(10.0.0.15)")]:::mgmt
        end

        subgraph Workload_Zone [Zone: Application Tier]
            Web["üåê web-srv1<br />(Nginx)"]:::compute
            App["‚öôÔ∏è app-srv1<br />(Backend)"]:::compute
        end

        subgraph Data_Zone [Zone: Persistence Tier]
            DB1[("üóÑÔ∏è data-001<br />(Primary)")]:::data
            DB2[("üóÑÔ∏è data-002<br />(Replica)")]:::data
        end

        subgraph Specialized_Compute [Zone: Virtualization]
            KVM["üñ•Ô∏è kvm-001<br />(Libvirt/QEMU)"]:::virt
            Docker["üê≥ docker-001<br />(Container Runtime)"]:::virt
        end

        %% Conexiones L√≥gicas (Flujo de Gesti√≥n)
        Bastion -.-> Web
        Bastion -.-> App
        Bastion -.-> DB1
        Bastion -.-> DB2
        Bastion -.-> KVM
        Bastion -.-> Docker

        %% Conexiones de Aplicaci√≥n
        Web --> App
        App --> DB1
        DB1 -.-> DB2
    </pre>
  </div>

  <div class="info-footer">
    <p>
      <strong>Topology Context:</strong> The diagram above represents the logical
      connectivity and role distribution of the reference cluster. All operations
      are initiated from the <code>terminal</code> node (Bastion Host) via SSH.
    </p>
  </div>
</div>

<style>
  .arch-card {
    border: 1px solid var(--sl-color-gray-5);
    background-color: var(--sl-color-gray-6);
    border-radius: 0.5rem;
    overflow: hidden;
    margin-bottom: 2rem;
    box-shadow: var(--sl-shadow-sm);
  }

  .card-header {
    background-color: var(--sl-color-gray-5);
    padding: 0.5rem 1rem;
    font-size: 0.9rem;
    border-bottom: 1px solid var(--sl-color-gray-4);
    display: flex;
    align-items: center;
    gap: 0.5rem;
    font-weight: 600;
    color: var(--sl-color-white);
  }

  .diagram-wrapper {
    background-color: #1e1e1e; /* Fondo oscuro para el diagrama */
    padding: 1rem;
    display: flex;
    justify-content: center;
  }

  .info-footer {
    padding: 1rem;
    font-size: 0.85rem;
    color: var(--sl-color-gray-2);
    border-top: 1px solid var(--sl-color-gray-5);
  }
</style>

```

---

## Contenido archivo: `src/components/QuizLoader.astro`

```bash
$ cat src/components/QuizLoader.astro
---
// RUTA: src/components/QuizLoader.astro (REFACTORIZADO PARA BOTONES)
import { getCollection } from 'astro:content';
import Quiz from './Quiz.astro';
const allQuestions = await getCollection('questions');
---

<div id="quiz-loader-wrapper">
  {allQuestions.map((question, index) => {
    // --- L√ìGICA NUEVA: Calculamos los IDs de navegaci√≥n ---
    const prevQuestionId = index > 0 ? allQuestions[index - 1].id : null;
    const nextQuestionId = index < allQuestions.length - 1 ? allQuestions[index + 1].id : null;

    return (
      <div id={question.id} class="quiz-instance-wrapper" style="display: none;">
        {/* Pasamos los nuevos props a Quiz.astro */}
        <Quiz 
          question={question.data} 
          questionId={question.id} 
          prevId={prevQuestionId} 
          nextId={nextQuestionId} 
        />
      </div>
    );
  })}
  <div id="quiz-not-found" style="display: none;">
    <h3>Question not found</h3>
    <p>Return to the <a href="/practice/">practice index</a>.</p>
  </div>
</div>

<!-- --- INICIO DEL SCRIPT ACTUALIZADO --- -->
<script>
  // Una √∫nica funci√≥n para manejar todos los eventos del formulario
  function handleQuizEvent(event) {
    const form = event.target.closest('.options-form');
    if (!form) return;

    // --- L√ìGICA NUEVA: Manejar el evento de reseteo ---
    if (event.type === 'reset') {
      const quizContainer = form.closest('.quiz-container');
      const feedbackContainer = quizContainer.querySelector('.feedback-container');
      feedbackContainer.style.display = 'none'; // Ocultamos el feedback
      // No hace falta preventDefault, el reset ya limpia el formulario
      return; 
    }

    // --- L√ìGICA EXISTENTE: Manejar el env√≠o del formulario ---
    if (event.type === 'submit') {
      event.preventDefault();

      const correctAnswerId = form.dataset.correctAnswerId;
      const explanation = JSON.parse(form.dataset.explanation);
      const quizContainer = form.closest('.quiz-container');
      const selectedOption = form.querySelector(`input[name^='quiz_option_']:checked`);
      
      if (!selectedOption) {
        alert('Please select an answer.');
        return;
      }

      const feedbackContainer = quizContainer.querySelector('.feedback-container');
      const feedbackTitle = feedbackContainer.querySelector('.feedback-title');
      const feedbackSummary = feedbackContainer.querySelector('.feedback-summary');
      const feedbackBreakdown = feedbackContainer.querySelector('.feedback-breakdown');

      if (selectedOption.value === correctAnswerId) {
        feedbackTitle.textContent = '‚úÖ Correct!';
        feedbackTitle.style.color = 'green';
      } else {
        feedbackTitle.textContent = '‚ùå Incorrect';
        feedbackTitle.style.color = 'red';
      }

      feedbackSummary.textContent = explanation.summary;
      feedbackBreakdown.innerHTML = '';
      explanation.breakdown.forEach(item => {
        const li = document.createElement('li');
        li.textContent = `${item.optionId}: ${item.reasoning}`;
        if (item.optionId === correctAnswerId) li.style.fontWeight = 'bold';
        if (item.optionId === selectedOption.value && selectedOption.value !== correctAnswerId) li.style.color = 'darkred';
        feedbackBreakdown.appendChild(li);
      });

      feedbackContainer.style.display = 'block';
    }
  }

  // --- L√ìGICA EXISTENTE: Mostrar el quiz correcto al cargar la p√°gina ---
  function showQuizFromUrl() {
    const params = new URLSearchParams(window.location.search);
    const questionId = params.get('id');

    if (questionId) {
      const quizWrapper = document.getElementById(questionId);
      if (quizWrapper) {
        quizWrapper.style.display = 'block';
      } else {
        document.getElementById('quiz-not-found').style.display = 'block';
      }
    } else {
      document.getElementById('quiz-not-found').style.display = 'block';
    }
  }

  // --- L√ìGICA NUEVA: A√±adimos 'reset' a la lista de eventos que escuchamos ---
  document.addEventListener('submit', handleQuizEvent);
  document.addEventListener('reset', handleQuizEvent);
  document.addEventListener('DOMContentLoaded', showQuizFromUrl);
</script>
<!-- --- FIN DEL SCRIPT ACTUALIZADO --- -->
```

---

## Contenido archivo: `src/content/docs/es/index.mdx`

```bash
$ cat src/content/docs/es/index.mdx
---
title: SysAdmin & DevOps
description: Portfolio de automatizaci√≥n de infraestructura y administraci√≥n de sistemas por Daniel Zamo.
template: splash
hero:
  tagline: Automatizaci√≥n de Infraestructura, Administraci√≥n Linux Enterprise y Arquitectura Cloud.
  image:
    file: ../../../assets/images/hero-image.png
  actions:
    - text: Ver Proyecto Destacado (LAMP)
      link: /es/projects/middleware/lamp-ansible-docker/
      icon: rocket
      variant: primary
    - text: Procedimientos Operativos
      link: /es/linux-ops/
      icon: right-arrow
      variant: minimal
prev: false
next: false
editUrl: false
lastUpdated: 2025-12-06
head:
  - tag: meta
    attrs:
      property: og:title
      content: "Daniel Zamo | SysAdmin & DevOps"
  - tag: meta
    attrs:
      property: og:description
      content: "Soluciones de Infraestructura como C√≥digo (Ansible), Docker y Administraci√≥n Linux Enterprise."
  - tag: meta
    attrs:
      property: og:image
      content: https://daniel-zamo.github.io/images/og-homepage-es-es.png
  - tag: meta
    attrs:
      property: og:image:width
      content: "1200"
  - tag: meta
    attrs:
      property: og:image:height
      content: "630"
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## Implementaciones y Competencias

<CardGrid stagger>
  <Card title="Automatizaci√≥n & IaC" icon="rocket">
    Despliegue de infraestructura inmutable. Ejemplo destacado: **Stack LAMP** con gesti√≥n de secretos, construcci√≥n de im√°genes Docker custom y Makefiles.
    
    **[Ver Caso de Estudio: LAMP + Ansible ‚Üí](/es/projects/middleware/lamp-ansible-docker/)**
  </Card>

  <Card title="Arquitectura Cloud Azure" icon="microsoft">
    Dise√±o de soluciones h√≠bridas y gobierno de identidad. Alineado con el est√°ndar **AZ-104** (Administrador de Azure).
    
    **[Explorar Operaciones Cloud ‚Üí](/es/cloud-ops/)**
  </Card>
  
  <Card title="Operaciones Linux Enterprise" icon="linux">
    Base de conocimiento t√©cnico para entornos de misi√≥n cr√≠tica: Gesti√≥n de almacenamiento LVM, Hardening y Troubleshooting.
    
    **[Consultar Procedimientos (SOP) ‚Üí](/es/linux-ops/)**
  </Card>
  
  <Card title="Orquestaci√≥n KVM/Libvirt" icon="laptop">
    Gesti√≥n de bajo nivel de recursos de c√≥mputo. Aprovisionamiento de m√°quinas virtuales (VMs) mediante CLI y optimizaci√≥n de rendimiento.
    
    **[Ver Escenario de Virtualizaci√≥n ‚Üí](/es/linux-ops/scenarios/04-kvm-management/)**
  </Card>

  <Card title="Arquitectura Cloud Azure" icon="microsoft">
    Dise√±o de soluciones h√≠bridas y gobierno de identidad. Alineado con el est√°ndar **AZ-104** (Administrador de Azure).
    
    *(Estado: Roadmap de Certificaci√≥n Activo)*
  </Card>
</CardGrid>

```

---

## Contenido archivo: `src/content/docs/es/linux-ops/index.mdx`

```bash
$ cat src/content/docs/es/linux-ops/index.mdx
---
title: Operaciones de Sistemas Linux (Enterprise)
description: Base de conocimiento t√©cnico para la administraci√≥n, hardening y optimizaci√≥n de sistemas Linux.
sidebar:
  label: Visi√≥n General
  order: 0
type: theory
source: manual
---

import { Card, CardGrid, LinkButton, Icon } from '@astrojs/starlight/components';

Este repositorio centraliza los **Procedimientos Operativos Est√°ndar (SOP)** y las mejores pr√°cticas de ingenier√≠a para la gesti√≥n de infraestructura basada en Linux.

El contenido refleja est√°ndares de producci√≥n, priorizando la estabilidad, la seguridad y la capacidad de auditor√≠a en entornos de misi√≥n cr√≠tica.

:::tip[Enfoque Operacional]
Esta documentaci√≥n prioriza la **CLI (L√≠nea de Comandos)** sobre herramientas gr√°ficas, garantizando la capacidad de gesti√≥n en servidores *headless* y la automatizaci√≥n de tareas.
:::

## üéØ Dominios de Competencia

√Åreas funcionales cr√≠ticas para la continuidad del servicio:

<CardGrid>
  <Card title="Operaciones Core" icon="linux">
    Manipulaci√≥n de flujos de texto, gesti√≥n de procesos y scripting en Bash.
  </Card>
  <Card title="Gesti√≥n de Runtime" icon="setting">
    An√°lisis de arranque (Systemd), par√°metros del kernel y tuning de sistema.
  </Card>
  <Card title="Gesti√≥n de Identidad" icon="user">
    Usuarios locales, integraci√≥n LDAP/AD y pol√≠ticas PAM.
  </Card>
  <Card title="Redes & Seguridad" icon="signal">
    Enrutamiento, Firewalling y sincronizaci√≥n NTP/PTP precisa.
  </Card>
  <Card title="Servicios de Infraestructura" icon="puzzle">
    DNS, SSH seguro, Servidores Web y Runtimes de contenedores.
  </Card>
  <Card title="Almacenamiento" icon="laptop">
    Gesti√≥n de LVM, RAID por software y sistemas de archivos transaccionales.
  </Card>
</CardGrid>

---

## üß™ Procedimientos Operativos

Acceda al cat√°logo de escenarios para ver implementaciones paso a paso de tareas de administraci√≥n.

<div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 1rem;">
    <LinkButton href="/es/linux-ops/scenarios/" variant="primary" icon="rocket">
        Ver Cat√°logo de Procedimientos
    </LinkButton>
</div>
```

---

## Contenido archivo: `src/content/docs/es/linux-ops/scenarios/index.mdx`

```bash
$ cat src/content/docs/es/linux-ops/scenarios/index.mdx
---
title: Cat√°logo de Escenarios Operativos
description: Colecci√≥n de procedimientos t√©cnicos para la resoluci√≥n de incidencias, aprovisionamiento de recursos y gesti√≥n de ciclo de vida en Linux.
sidebar:
  label: Cat√°logo de Escenarios
  order: 0
type: scenario
source: manual
---

import { CardGrid, LinkCard, Icon } from '@astrojs/starlight/components';

Esta secci√≥n aloja la **capa de ejecuci√≥n pr√°ctica** del portfolio. Cada entrada est√° estructurada como un **Procedimiento Operativo Est√°ndar (SOP)**, simulando tickets de infraestructura reales que requieren configuraci√≥n, *troubleshooting* y validaci√≥n en un entorno vivo.

El objetivo es demostrar capacidad de resoluci√≥n bajo restricciones t√©cnicas alineadas con est√°ndares de la industria (LFCS/RHCSA).

---

## üèóÔ∏è Topolog√≠a del Entorno (Standard Operating Environment)

Todos los escenarios se ejecutan sobre un cl√∫ster estandarizado de 7 nodos. Esta topolog√≠a simula una arquitectura de tres capas (Web, App, Data) con capacidades de virtualizaci√≥n anidadas.

:::danger[Protocolos de Acceso y Seguridad]
*   **Basti√≥n:** Todas las operaciones se inician desde el nodo `terminal`.
*   **SSH Anidado:** Estrictamente prohibido por pol√≠ticas de seguridad. La conexi√≥n debe ser directa desde el controlador.
*   **Persistencia:** Toda configuraci√≥n debe sobrevivir al reinicio del sistema (*Reboot-proof*).
:::

### Inventario de Nodos

| Rol | Hostname | Funci√≥n / Contexto de Negocio |
| :--- | :--- | :--- |
| **Gesti√≥n** | `terminal` | Controlador de operaciones (Bastion Host) y CLI principal. |
| **Capa Web** | `web-srv1` | Terminaci√≥n SSL, Proxy Inverso y Servidor HTTP. |
| **Capa App** | `app-srv1` | Ejecuci√≥n de l√≥gica de negocio y APIs internas. |
| **Capa Datos** | `data-001/002` | Persistencia, bases de datos y almacenamiento en bloque. |
| **Virtualizaci√≥n** | `kvm-001` | Hipervisor Libvirt para consolidaci√≥n de servidores. |
| **Contenedores** | `docker-001` | Runtime para microservicios y orquestaci√≥n ligera. |

---

## üöÄ Procedimientos Disponibles

Seleccione un escenario para ver los requisitos de ingenier√≠a y la estrategia de implementaci√≥n.

<CardGrid>
  <LinkCard
    title="01 - Gesti√≥n de Tiempo (NTP/Chrony)"
    description="Configuraci√≥n de clientes de tiempo para garantizar la consistencia transaccional en sistemas distribuidos."
    href="/es/linux-ops/scenarios/01-time-sync/"
  />
  <LinkCard
    title="02 - Balanceo de Carga L7 (Nginx)"
    description="Implementaci√≥n de Proxy Reverso y balanceo de tr√°fico HTTP para aplicaciones internas."
    href="/es/linux-ops/scenarios/02-nginx-proxy/"
  />
  <LinkCard
    title="03 - Storage & LVM"
    description="Gesti√≥n el√°stica de vol√∫menes: Redimensionamiento en caliente y gesti√≥n de Physical Volumes."
    href="/es/linux-ops/scenarios/03-storage-mgmt/"
  />
   <LinkCard
    title="04 - Orquestaci√≥n KVM"
    description="Aprovisionamiento de m√°quinas virtuales (VMs) y gesti√≥n del ciclo de vida con Virsh."
    href="/es/linux-ops/scenarios/04-kvm-management/"
  />
</CardGrid>

:::tip[Nota de Infraestructura]
Los procedimientos asumen conectividad de red preconfigurada entre los nodos del inventario. Para detalles espec√≠ficos de direccionamiento IP, consulte la ficha t√©cnica dentro de cada escenario.
:::

```

---

## Contenido archivo: `src/content/docs/es/linux-ops/scenarios/04-kvm-management.mdx`

```bash
$ cat src/content/docs/es/linux-ops/scenarios/04-kvm-management.mdx
---
title: "Orquestaci√≥n de VMs con KVM/Libvirt"
description: Aprovisionamiento y gesti√≥n del ciclo de vida de instancias de c√≥mputo utilizando herramientas CLI de virtualizaci√≥n.
sidebar:
  label: Orquestaci√≥n KVM
  order: 4
type: scenario
domain: virtualization
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Requisitos de Ingenier√≠a

**Contexto de Negocio:**
El equipo de QA requiere una instancia de c√≥mputo dedicada y aislada en el hipervisor `kvm-001` para pruebas de integraci√≥n. El entorno debe ser aprovisionado usando herramientas CLI est√°ndar para asegurar la reproducibilidad.

**Objetivos T√©cnicos:**
1.  **Estado del Host:** Validar el estado del stack `qemu-kvm` y `libvirtd`.
2.  **Almacenamiento:** Aprovisionar un volumen `qcow2` de **10 GiB** (Thin Provisioned) llamado `qa-vm-disk.qcow2`.
3.  **Definici√≥n de Instancia (`qa-vm01`):**
    *   **Specs:** 1 vCPU, 1024 MiB RAM.
    *   **Red:** Bridge NAT por defecto.
    *   **Consola:** Redirecci√≥n serie (Modo Headless).
4.  **Ciclo de Vida:** Habilitar auto-inicio en el arranque y realizar prueba de apagado controlado.

:::note[Alcance]
No se requiere la instalaci√≥n completa del OS. Arrancar exitosamente el kernel hasta el prompt del instalador v√≠a consola serie es suficiente para la validaci√≥n.
:::

---

## üîß Procedimiento de Implementaci√≥n

<Steps>

1.  **Acceso al Nodo Hipervisor**

    Conecte al host de virtualizaci√≥n.

    ```bash
    ssh kvm-001
    ```

2.  **Verificar Prerrequisitos del Host**

    Aseg√∫rese de que el demonio libvirt est√° activo y habilitado.

    ```bash
    sudo systemctl is-active libvirtd
    # Si inactivo: sudo systemctl enable --now libvirtd
    ```

3.  **Aprovisionar Volumen de Almacenamiento**

    Cree el disco virtual usando `qemu-img`.

    ```bash
    sudo qemu-img create -f qcow2 /var/lib/libvirt/images/qa-vm-disk.qcow2 10G
    ```

4.  **Definir y Arrancar Instancia**

    Ejecute `virt-install` con par√°metros para entorno headless (`--graphics none`).

    ```bash
    sudo virt-install \
      --name qa-vm01 \
      --ram 1024 \
      --vcpus 1 \
      --disk path=/var/lib/libvirt/images/qa-vm-disk.qcow2,format=qcow2 \
      --os-variant rocky9 \
      --network network=default \
      --graphics none \
      --console pty,target_type=serial \
      --location "https://download.rockylinux.org/pub/rocky/9/BaseOS/x86_64/os/" \
      --extra-args "console=ttyS0"
    ```

    *Tip: Para desconectarse de la consola sin matar la VM, presione `Ctrl + ]`.*

5.  **Configurar Auto-inicio**

    Asegure que la VM sobrevive a reinicios del host.

    ```bash
    sudo virsh autostart qa-vm01
    ```

6.  **Validar Control de Ciclo de Vida**

    Verifique que la VM corre, luego solicite un apagado ACPI.

    ```bash
    # Verificar estado
    sudo virsh list --all

    # Solicitud de apagado controlado
    sudo virsh shutdown qa-vm01
    ```

</Steps>

## üîç Diagn√≥stico y Resoluci√≥n

*   **"Guest name already in use":** Si un intento previo fall√≥ parcialmente, limpie los metadatos:
    ```bash
    sudo virsh destroy qa-vm01
    sudo virsh undefine qa-vm01
    ```
*   **Consola Congelada:** Si la consola no muestra salida, aseg√∫rese de que `--extra-args "console=ttyS0"` fue pasado correctamente para redirigir los logs del kernel al puerto serie.

```

---

## Contenido archivo: `src/content/docs/es/linux-ops/scenarios/01-time-sync.mdx`

```bash
$ cat src/content/docs/es/linux-ops/scenarios/01-time-sync.mdx
---
title: "Estrategia NTP y Sincronizaci√≥n"
description: Configuraci√≥n de pol√≠ticas systemd-timesyncd para consistencia transaccional en cl√∫sters distribuidos.
sidebar:
  label: Estrategia NTP
  order: 1
type: scenario
domain: networking
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Requisitos de Ingenier√≠a

**Contexto de Negocio:**
Los registros de auditor√≠a indican una deriva (drift) en los relojes del sistema a trav√©s del cl√∫ster, causando fallos en la conciliaci√≥n de transacciones distribuidas. Se debe aplicar una pol√≠tica estricta de NTP en el nodo controlador.

**Objetivos T√©cnicos:**
1.  **Upstreams Primarios:** Configurar `0.pool.ntp.org` y `1.pool.ntp.org` como fuentes autoritativas.
2.  **Failover:** Definir `ntp.ubuntu.com` y `0.debian.pool.ntp.org` como servidores de respaldo.
3.  **Ajuste de Rendimiento (Tuning):**
    *   Intervalo m√°ximo de sondeo (Poll): `1000s`.
    *   Intervalo de reintento de conexi√≥n: `20s`.

:::caution[Restricci√≥n de Servicio]
El entorno utiliza `systemd-timesyncd` como cliente SNTP ligero est√°ndar. No instale paquetes `chrony` o `ntp` legacy a menos que se requiera expl√≠citamente, para evitar conflictos de servicio.
:::

---

## üîß Procedimiento de Implementaci√≥n

<Steps>

1.  **Auditor√≠a del Estado Actual**

    Verifique la versi√≥n del sistema operativo y el estado activo del servicio de tiempo antes de aplicar cambios.

    ```bash
    # Verificar versi√≥n de OS
    cat /etc/os-release | grep PRETTY_NAME

    # Verificar estado del servicio
    timedatectl timesync-status
    ```

2.  **Aplicar Pol√≠tica de Configuraci√≥n**

    Edite el archivo de configuraci√≥n del demonio `/etc/systemd/timesyncd.conf`.

    ```bash
    sudo vim /etc/systemd/timesyncd.conf
    ```

    Modifique la secci√≥n `[Time]` para coincidir estrictamente con los requisitos:

    ```ini title="/etc/systemd/timesyncd.conf" {2-5}
    [Time]
    NTP=0.pool.ntp.org 1.pool.ntp.org
    FallbackNTP=ntp.ubuntu.com 0.debian.pool.ntp.org
    PollIntervalMaxSec=1000
    ConnectionRetrySec=20
    ```

3.  **Reiniciar Servicio**

    Aplique la nueva pol√≠tica reiniciando la unidad systemd.

    ```bash
    sudo systemctl restart systemd-timesyncd.service
    ```

4.  **Validar Implementaci√≥n**

    Confirme que el servicio est√° activo y sincronizando con el estrato (stratum) correcto.

    ```bash
    # Estado del servicio
    sudo systemctl status systemd-timesyncd.service

    # Validar m√©tricas (Poll interval)
    timedatectl timesync-status
    ```

    *Criterio de Validaci√≥n:* La salida debe mostrar `Active: active (running)` y las m√©tricas de intervalo coincidiendo con la configuraci√≥n.

</Steps>

## üîç Diagn√≥stico y Resoluci√≥n

Si la sincronizaci√≥n falla (ej. "Packet not received"), inspeccione el journal del sistema:

```bash
sudo journalctl -u systemd-timesyncd --no-pager | tail -n 20
```

*Indicador de √âxito:* Busque logs que indiquen: *"Contacted time server ... (0.pool.ntp.org)"*.

```

---

## Contenido archivo: `src/content/docs/es/linux-ops/scenarios/05-kvm-networking.mdx`

```bash
$ cat src/content/docs/es/linux-ops/scenarios/05-kvm-networking.mdx
---
title: "Networking Avanzado KVM: Bridges & VLANs"
description: Dise√±o e implementaci√≥n de segmentaci√≥n de red 802.1Q y Linux Bridges persistentes sobre host de virtualizaci√≥n.
sidebar:
  label: Networking KVM (VLANs)
  order: 5
type: scenario
domain: networking
---

import { Steps, LinkButton } from '@astrojs/starlight/components';

## üìã Definici√≥n de Arquitectura

**Contexto de Ingenier√≠a:**
En entornos de virtualizaci√≥n *bare-metal* (Host KVM), el uso de una red plana ("Flat Network") presenta riesgos de seguridad y congesti√≥n de broadcast. Se requiere evolucionar la configuraci√≥n de red del host anfitri√≥n para soportar **m√∫ltiples zonas de aislamiento** utilizando una √∫nica interfaz f√≠sica (Trunk).

**Objetivos de Dise√±o:**
1.  **Desacoplamiento L3:** La IP de gesti√≥n del host no debe residir en la interfaz f√≠sica, sino en un Bridge virtual, permitiendo mayor flexibilidad.
2.  **Segmentaci√≥n 802.1Q:** Implementaci√≥n de sub-interfaces VLAN para segregar tr√°fico de Gesti√≥n (VLAN 19) y Laboratorio/Tenant (VLAN 88).
3.  **Persistencia NetworkManager:** Configuraci√≥n declarativa mediante `nmcli` para garantizar la estabilidad tras reinicios, abandonando scripts legacy.

---

## üèóÔ∏è Topolog√≠a L√≥gica del Host

El siguiente diagrama detalla la pila de red interna configurada dentro del Host KVM (`kvm-001`).

```mermaid
graph TD
    subgraph Physical_Layer [Capa F√≠sica]
        ENO1[("Interfaz F√≠sica<br/>(eno1 - Trunk)")]:::phys
    end

    subgraph VLAN_Layer [Capa 802.1Q - Tagging]
        VLAN19["eno1.19<br/>(VLAN ID: 19)"]:::vlan
        VLAN88["eno1.88<br/>(VLAN ID: 88)"]:::vlan
    end

    subgraph Bridge_Layer [Capa L2 - Switching Virtual]
        BR19["br19<br/>(Bridge Gesti√≥n)"]:::bridge
        BR88["br88<br/>(Bridge Cargas de Trabajo)"]:::bridge
    end

    subgraph IP_Layer [Capa L3 - Routing]
        IP_HOST["IP Host KVM<br/>192.168.19.50/24"]:::ip
        VM_MGMT["VM Controlador"]:::vm
        VM_LAB["VM Aplicaci√≥n"]:::vm
    end

    ENO1 ==> VLAN19
    ENO1 ==> VLAN88
    
    VLAN19 --> BR19
    VLAN88 --> BR88
    
    BR19 -.-> IP_HOST
    BR19 --> VM_MGMT
    BR88 --> VM_LAB

    %% Estilos Profesionales
    classDef phys fill:#e3f2fd,stroke:#1565c0,color:#0d47a1,stroke-width:2px;
    classDef vlan fill:#fff9c4,stroke:#fbc02d,color:#000;
    classDef bridge fill:#c8e6c9,stroke:#2e7d32,color:#1b5e20,stroke-width:2px;
    classDef ip fill:#ffccbc,stroke:#d84315,color:#bf360c;
    classDef vm fill:#f5f5f5,stroke:#616161,color:#000;
```

---

## üöÄ Procedimiento de Implementaci√≥n (SOP)

Este dise√±o ha sido estandarizado y documentado en la **Librer√≠a de Est√°ndares de Operaciones**. El SOP detalla los comandos at√≥micos de `nmcli` necesarios para transformar una interfaz est√°ndar en un trunk VLAN con bridges.

<div style="display: flex; justify-content: center; gap: 1rem; margin: 2rem 0;">
  <LinkButton 
    href="https://github.com/daniel-zamo/ops-standards-library/blob/main/linux/SOP-LINUX-NM-01_bridge-vlan-config.md"
    variant="primary"
    icon="github"
    target="_blank"
  >
    Ver Procedimiento T√©cnico (GitHub)
  </LinkButton>
  <LinkButton 
    href="https://github.com/daniel-zamo/ops-standards-library/tree/main/linux"
    variant="secondary"
    icon="external"
    target="_blank"
  >
    Explorar Repo OPS
  </LinkButton>
</div>

### Resultados de la Configuraci√≥n

Al aplicar esta arquitectura:

1.  **Aislamiento:** Las VMs conectadas a `br88` est√°n l√≥gicamente aisladas de la red de gesti√≥n del host.
2.  **Estandarizaci√≥n:** Se elimina la dependencia de nombres de interfaz impredecibles (ej. `eth0` vs `enp3s0`) al abstraer la conexi√≥n en Bridges estandarizados (`brXX`).
3.  **Observabilidad:** La estructura permite inspeccionar el tr√°fico de cada segmento independientemente usando `tcpdump -i br19`.

```

---

## Contenido archivo: `src/content/docs/es/linux-ops/scenarios/03-storage-mgmt.mdx`

```bash
$ cat src/content/docs/es/linux-ops/scenarios/03-storage-mgmt.mdx
---
title: "Gesti√≥n del Ciclo de Vida LVM"
description: Redimensionamiento din√°mico de Grupos de Vol√∫menes y retiro de Vol√∫menes F√≠sicos en producci√≥n.
sidebar:
  label: Gesti√≥n LVM
  order: 3
type: scenario
domain: storage
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Requisitos de Ingenier√≠a

**Contexto de Negocio:**
Una iniciativa de consolidaci√≥n de almacenamiento requiere retirar el disco f√≠sico `/dev/vdh` del grupo de vol√∫menes existente `vol1`. Este disco debe ser reutilizado para crear un nuevo grupo de almacenamiento aislado (`vol2`) para un proyecto espec√≠fico.

**Objetivos T√©cnicos:**
1.  **Migraci√≥n en Caliente:** Evacuar datos de `/dev/vdh` sin interrupci√≥n del servicio usando capacidades de mirroring de LVM.
2.  **Cambio de Topolog√≠a:** Remover `/dev/vdh` de `vol1` e inicializar un nuevo Volume Group `vol2`.
3.  **Aprovisionamiento:** Crear un Volumen L√≥gico `p1` (50 MiB) dentro de `vol2`.
4.  **Filesystem:** Formatear el nuevo volumen con **ext4**.

:::caution[Integridad de Datos]
En producci√≥n, ejecutar `pvmove` es una operaci√≥n intensiva en I/O. Aseg√∫rese de que no haya backups cr√≠ticos ejecut√°ndose simult√°neamente.
:::

---

## üîß Procedimiento de Implementaci√≥n

<Steps>

1.  **Analizar Topolog√≠a de Almacenamiento**

    Confirme el esquema actual y verifique que `/dev/vdh` es parte de `vol1`.

    ```bash
    sudo pvs
    sudo vgs
    ```

2.  **Evacuaci√≥n de Datos (pvmove)**

    Reubique los "physical extents" (PE) asignados desde el disco objetivo hacia otros discos libres dentro del mismo Grupo de Vol√∫menes.

    ```bash
    # Verificar asignaci√≥n
    sudo pvdisplay -m /dev/vdh

    # Migrar datos
    sudo pvmove /dev/vdh
    ```

3.  **Retirar Disco**

    Una vez el disco est√© totalmente libre, remu√©valo del Grupo de Vol√∫menes.

    ```bash
    sudo vgreduce vol1 /dev/vdh
    ```

4.  **Inicializar Nuevo Grupo de Almacenamiento**

    Cree el nuevo Volume Group `vol2` usando el volumen f√≠sico reclamado.

    ```bash
    sudo vgcreate vol2 /dev/vdh
    ```

5.  **Aprovisionar Volumen L√≥gico**

    Asigne la capacidad de almacenamiento solicitada.

    ```bash
    sudo lvcreate -n p1 -L 50M vol2
    ```

6.  **Aplicar Sistema de Archivos**

    Formatee el dispositivo de bloque.

    ```bash
    sudo mkfs.ext4 /dev/vol2/p1
    ```

7.  **Validaci√≥n Final**

    Verifique la nueva estructura LVM y la firma del filesystem.

    ```bash
    sudo lvs -o lv_name,vg_name,lv_size,seg_pe_ranges
    lsblk -f /dev/vol2/p1
    ```

</Steps>

## üîç Diagn√≥stico y Resoluci√≥n

*   **"Physical volume is still in use":** El comando `vgreduce` fallar√° si salt√≥ el Paso 2. **Debe** evacuar los datos primero.
*   **"Insufficient free extents":** Si `vol1` no tiene suficiente espacio libre en otros discos (ej. `/dev/vdb`) para contener los datos de `/dev/vdh`, la operaci√≥n `pvmove` fallar√°.

```

---

## Contenido archivo: `src/content/docs/es/linux-ops/scenarios/02-nginx-proxy.mdx`

```bash
$ cat src/content/docs/es/linux-ops/scenarios/02-nginx-proxy.mdx
---
title: "Balanceo de Carga L7 y Edge Proxy"
description: Implementaci√≥n de reglas de Proxy Reverso y algoritmos de distribuci√≥n de tr√°fico usando Nginx.
sidebar:
  label: Balanceo de Carga L7
  order: 2
type: scenario
domain: networking
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Requisitos de Ingenier√≠a

**Contexto de Negocio:**
Las aplicaciones internas legacy alojadas en `web-server01` est√°n expuestas actualmente en puertos no est√°ndar (`1111`, `2222`), complicando las reglas de firewall y el acceso del cliente. Se requiere una soluci√≥n de Proxy Reverso para estandarizar el acceso v√≠a HTTP.

**Objetivos T√©cnicos:**
1.  **Nodo Objetivo:** `web-server01` (10.0.0.60).
2.  **Redirecci√≥n de Tr√°fico (Puerto 8001):**
    *   El tr√°fico entrante debe ser reenviado a la ruta backend espec√≠fica: `http://127.0.0.1:2222/special`.
3.  **Balanceo de Carga (Puerto 8000):**
    *   Distribuir tr√°fico entre **App A** (`:1111`) y **App B** (`:2222`).
    *   Algoritmo: Round Robin (Por defecto) o Random.

:::danger[Restricciones Operativas]
*   **Backend Inmutable:** No modifique los archivos de configuraci√≥n de Nginx existentes que sirven los puertos 1111/2222.
*   **Persistencia:** La configuraci√≥n debe persistir tras un reinicio del sistema.
:::

---

## üîß Procedimiento de Implementaci√≥n

<Steps>

1.  **Acceso al Nodo Objetivo**

    Establezca una conexi√≥n segura al servidor web desde el host basti√≥n.

    ```bash
    ssh web-server01
    ```

2.  **Verificar Salud del Backend**

    Aseg√∫rese de que las aplicaciones upstream est√°n operativas antes de configurar el proxy.

    ```bash
    curl -I http://127.0.0.1:1111
    curl -I http://127.0.0.1:2222
    ```

3.  **Definir Configuraci√≥n del Proxy**

    Cree un archivo de configuraci√≥n modular en `/etc/nginx/conf.d/`. Evite editar `nginx.conf` directamente.

    ```bash
    sudo vim /etc/nginx/conf.d/loadbalancer.conf
    ```

    Implemente el siguiente bloque para definir el grupo upstream y los contextos server:

    ```nginx title="/etc/nginx/conf.d/loadbalancer.conf"
    # 1. Definir Grupo Upstream (Balanceador)
    upstream backend_apps {
        server 127.0.0.1:1111;
        server 127.0.0.1:2222;
    }

    # 2. Tarea: Balanceador de Carga (Puerto 8000)
    server {
        listen 8000;
        server_name localhost;

        location / {
            proxy_pass http://backend_apps;
        }
    }

    # 3. Tarea: Redirecci√≥n/Proxy (Puerto 8001)
    server {
        listen 8001;
        server_name localhost;

        location / {
            proxy_pass http://127.0.0.1:2222/special;
        }
    }
    ```

4.  **Validar y Recargar**

    Realice una comprobaci√≥n de sintaxis para prevenir ca√≠das del servicio, luego recargue la se√±al del proceso.

    ```bash
    sudo nginx -t
    sudo systemctl reload nginx
    ```

5.  **Verificaci√≥n Funcional**

    **Probar Balanceador (8000):** Ejecute m√∫ltiples peticiones. Las respuestas deben alternar entre backends.
    ```bash
    for i in {1..4}; do curl http://localhost:8000; echo; done
    ```

    **Probar Redirecci√≥n (8001):**
    ```bash
    curl -v http://localhost:8001
    ```

</Steps>

## üîç Diagn√≥stico y Resoluci√≥n

*   **Connection Refused:** Aseg√∫rese de que Nginx est√° escuchando en los nuevos puertos:
    ```bash
    ss -tulpn | grep nginx
    ```
*   **502 Bad Gateway:** Indica que Nginx no puede alcanzar los puertos upstream (1111/2222). Verifique si las apps backend han ca√≠do.

```

---

## Contenido archivo: `src/content/docs/es/cloud-ops/index.mdx`

```bash
$ cat src/content/docs/es/cloud-ops/index.mdx
---
title: Arquitectura & Operaciones Azure
description: Documentaci√≥n t√©cnica para el dise√±o, gobierno y administraci√≥n de entornos Cloud basados en Microsoft Azure.
sidebar:
  label: Visi√≥n General
  order: 0
type: theory
domain: cloud
---

import { Card, CardGrid, LinkButton } from '@astrojs/starlight/components';

Esta secci√≥n consolida **procedimientos de ingenier√≠a** y arquitecturas de referencia para la implementaci√≥n de infraestructura en la nube.

El contenido se alinea con las mejores pr√°cticas del **Microsoft Cloud Adoption Framework (CAF)**, priorizando la seguridad, la automatizaci√≥n y el gobierno operativo sobre la gesti√≥n manual.

---

## ‚òÅÔ∏è Dominios de Arquitectura

<CardGrid>
  <Card title="Identidad y Gobierno" icon="user">
    Implementaci√≥n de pol√≠ticas de acceso (RBAC), gesti√≥n de identidades con Entra ID y cumplimiento normativo mediante Azure Policy.
  </Card>
  <Card title="Networking (VNETs)" icon="signal">
    Dise√±o de topolog√≠as Hub-and-Spoke, segmentaci√≥n de tr√°fico con NSGs y conectividad h√≠brida segura.
  </Card>
  <Card title="C√≥mputo IaaS" icon="laptop">
    Gesti√≥n del ciclo de vida de M√°quinas Virtuales, optimizaci√≥n de costes (Spot Instances) y escalado autom√°tico.
  </Card>
  <Card title="Storage & Data" icon="document">
    Administraci√≥n de cuentas de almacenamiento, protecci√≥n de datos y estrategias de redundancia (LRS/GRS).
  </Card>
</CardGrid>

---

## üöÄ Implementaciones T√©cnicas

Escenarios pr√°cticos de despliegue y resoluci√≥n de problemas de infraestructura.

<div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 1rem;">
    <LinkButton href="/es/cloud-ops/compute/scenario-01-lamp-iaas/" variant="primary" icon="rocket">
        Ver Escenario 01: Stack LAMP IaaS
    </LinkButton>
</div>

```

---

## Contenido archivo: `src/content/docs/es/cloud-ops/compute/scenario-01-lamp-iaas.mdx`

```bash
$ cat src/content/docs/es/cloud-ops/compute/scenario-01-lamp-iaas.mdx
---
title: "Escenario 01: Migraci√≥n LAMP a Azure IaaS"
description: Despliegue automatizado de un servidor web Linux (Apache/PHP/MySQL) en Azure usando CLI y Cloud-Init.
sidebar:
  label: 01 - Stack LAMP IaaS
  order: 1
type: scenario
domain: compute
---

import { Steps } from '@astrojs/starlight/components';

## üìã Definici√≥n del Proyecto

**Contexto de Negocio:**
Como parte de una iniciativa de modernizaci√≥n, se requiere migrar una carga de trabajo web legacy (Stack LAMP) a la nube p√∫blica. El objetivo es reducir la sobrecarga operativa del hardware f√≠sico manteniendo el control total sobre la configuraci√≥n del sistema operativo.

**Requisitos T√©cnicos:**
1.  **Aislamiento de Recursos:** Implementaci√≥n de un Grupo de Recursos dedicado para la gesti√≥n del ciclo de vida.
2.  **Seguridad de Red:** Segmentaci√≥n mediante VNET y filtrado de tr√°fico estricto (NSG) permitiendo √∫nicamente HTTP (80) y SSH (22).
3.  **Eficiencia de C√≥mputo:** Aprovisionamiento de instancia `Standard_B1s` (Burstable) sobre Ubuntu 22.04 LTS.
4.  **Automatizaci√≥n (Infrastructure as Code):** El aprovisionamiento debe ser desatendido, utilizando **Cloud-Init** para la configuraci√≥n del stack de software en el primer arranque.

:::tip[Estrategia de Aprovisionamiento]
En entornos productivos, se evita la configuraci√≥n manual post-despliegue. El uso de **Custom Data (Cloud-Init)** garantiza que la VM est√© operativa y configurada desde el segundo cero, facilitando la escalabilidad horizontal.
:::

---

## üîß Procedimiento de Implementaci√≥n (Azure CLI)

Ejecuci√≥n mediante `az cli` para garantizar la reproducibilidad y auditabilidad del despliegue.

<Steps>

1.  **Definici√≥n de Variables de Entorno**

    Estandarizaci√≥n de nomenclaturas para recursos.

    ```bash
    RG_NAME="rg-lamp-migration-001"
    LOCATION="northeurope"
    VM_NAME="vm-web-legacy-01"
    VNET_NAME="vnet-prod-01"
    NSG_NAME="nsg-web-public"
    ```

2.  **Creaci√≥n del Contenedor L√≥gico**

    ```bash
    az group create --name $RG_NAME --location $LOCATION
    ```

3.  **Configuraci√≥n de Bootstrapping (Cloud-Init)**

    Definici√≥n del estado deseado del sistema operativo. Este manifiesto instala dependencias y genera archivos de prueba autom√°ticamente.

    ```yaml title="cloud-init-lamp.txt"
    #cloud-config
    package_upgrade: true
    packages:
      - apache2
      - php
      - libapache2-mod-php
      - mysql-server
      - php-mysql
    write_files:
      - content: |
          <?php phpinfo(); ?>
        path: /var/www/html/info.php
    runcmd:
      - systemctl restart apache2
      - systemctl enable apache2
    ```

4.  **Despliegue de Infraestructura**

    Comando at√≥mico que aprovisiona red, seguridad, IP p√∫blica y c√≥mputo, inyectando la configuraci√≥n de Cloud-Init.

    ```bash
    az vm create \
      --resource-group $RG_NAME \
      --name $VM_NAME \
      --image Ubuntu2204 \
      --admin-username azureuser \
      --generate-ssh-keys \
      --public-ip-sku Standard \
      --size Standard_B1s \
      --custom-data @cloud-init-lamp.txt \
      --nsg $NSG_NAME
    ```

5.  **Hardening de Red (NSG)**

    Configuraci√≥n expl√≠cita de reglas de entrada para permitir tr√°fico web.

    ```bash
    az network nsg rule create \
      --resource-group $RG_NAME \
      --nsg-name $NSG_NAME \
      --name Allow-HTTP \
      --protocol Tcp \
      --priority 100 \
      --destination-port-range 80 \
      --access Allow
    ```

6.  **Verificaci√≥n de Servicio**

    Validaci√≥n de la disponibilidad del endpoint p√∫blico.

    ```bash
    IP_ADDRESS=$(az vm show -d -g $RG_NAME -n $VM_NAME --query publicIps -o tsv)
    echo "Endpoint disponible en: http://$IP_ADDRESS/info.php"
    ```

</Steps>

## üîç Decisi√≥n de Dise√±o

### ¬øPor qu√© Cloud-Init para este escenario?
Para despliegues de **instancias √∫nicas** o **im√°genes base**, Cloud-Init es el est√°ndar de la industria "Cloud-Native". Elimina la necesidad de un servidor de configuraci√≥n externo (como Ansible Tower) para el bootstrap inicial, reduciendo la complejidad y el tiempo de arranque (RTO).

```

---

## Contenido archivo: `src/content/docs/es/projects/index.mdx`

```bash
$ cat src/content/docs/es/projects/index.mdx
---
title: Portafolio de Ingenier√≠a
description: Iniciativas de automatizaci√≥n de infraestructura, pr√°cticas DevOps y arquitecturas Cloud-Native.
sidebar:
  label: Visi√≥n General
  order: 0
type: project
---

import { Card, CardGrid, LinkButton } from '@astrojs/starlight/components';

Esta secci√≥n consolida **proyectos de grado de producci√≥n** que demuestran experiencia pr√°ctica en la resoluci√≥n de problemas complejos de infraestructura.

Cada entrada est√° documentada no solo a nivel de c√≥digo, sino tambi√©n justificando las **decisiones de arquitectura** y el **impacto en el negocio**.

---

## üéØ Dominios de Especializaci√≥n

<CardGrid>
  <Card title="Automatizaci√≥n Middleware" icon="puzzle">
    Orquestaci√≥n de servidores de aplicaciones empresariales y reducci√≥n de deuda t√©cnica mediante Infraestructura como C√≥digo (IaC).
    
    **Stack:** Ansible, Docker, JBoss/WildFly.
    
    [Ver Proyectos Middleware ‚Üí](/es/projects/middleware/)
  </Card>
  
  <Card title="Arquitectura Cloud & Gobierno" icon="rocket">
    **Roadmap Activo:** Dise√±o de Azure Landing Zones, gesti√≥n de identidad (IAM) y redes h√≠bridas. Implementaci√≥n de referencia alineada con la certificaci√≥n **AZ-104**.
    
    **Stack:** Azure Resource Manager, VNet, Entra ID.
  </Card>
  
  <Card title="C√≥mputo & Virtualizaci√≥n" icon="laptop">
    **Roadmap Activo:** Automatizaci√≥n de hipervisores KVM/Libvirt y estrategias de aprovisionamiento de m√°quinas virtuales (VMs).
    
    **Stack:** QEMU/KVM, Virsh, Cloud-Init.
  </Card>
  
  <Card title="Ingenier√≠a de Release (CI/CD)" icon="setting">
    **Roadmap Activo:** Pipelines de integraci√≥n continua, gesti√≥n de artefactos y estrategias de despliegue automatizado.
    
    **Stack:** GitLab CI, Registros de Contenedores.
  </Card>
</CardGrid>

---

## üìä Habilidades Transversales

Estos proyectos validan competencias m√°s all√° de la ejecuci√≥n de comandos:

- **Infrastructure as Code (IaC):** Gesti√≥n declarativa de recursos para asegurar la consistencia.
- **Inmutabilidad:** Adopci√≥n de patrones de contenedores para eliminar la deriva de configuraci√≥n.
- **Modernizaci√≥n:** Capacidad para transformar sistemas legacy en arquitecturas gestionables.
- **Documentaci√≥n T√©cnica:** Comunicaci√≥n efectiva de dise√±os complejos para equipos de operaciones.

---

## üöÄ Navegaci√≥n R√°pida

<div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 1.5rem;">
  <LinkButton href="/es/projects/middleware/" variant="primary" icon="right-arrow">
    Soluciones Middleware
  </LinkButton>
  <LinkButton href="/es/lfcs/" variant="secondary" icon="document">
    Explorar Operaciones Linux
  </LinkButton>
</div>

:::tip[Contexto Profesional]
Los escenarios presentados reflejan desaf√≠os reales encontrados en entornos corporativos (Banca, Telco), priorizando la **seguridad**, la **mantenibilidad** y la **excelencia operacional**.
:::

```

---

## Contenido archivo: `src/content/docs/es/projects/middleware/index.mdx`

```bash
$ cat src/content/docs/es/projects/middleware/index.mdx
---
title: Ingenier√≠a de Middleware y Automatizaci√≥n
description: Estrategias de modernizaci√≥n para servidores de aplicaciones empresariales mediante IaC y contenedorizaci√≥n.
sidebar:
  label: Middleware
  order: 0
type: project
---

import { CardGrid, LinkCard } from '@astrojs/starlight/components';

## Visi√≥n Arquitect√≥nica

Esta secci√≥n documenta la **modernizaci√≥n de plataformas middleware**. El objetivo es transicionar infraestructuras monol√≠ticas y manuales hacia arquitecturas el√°sticas y gestionadas por c√≥digo.

### El Problema de Negocio

En sectores regulados, la gesti√≥n tradicional de servidores de aplicaciones presenta fricciones cr√≠ticas:
*   **Riesgo Operacional:** La configuraci√≥n manual introduce errores humanos y deriva entre entornos.
*   **Coste de Mantenimiento:** El tiempo dedicado a parches manuales impide la innovaci√≥n.

**Soluci√≥n:** Infraestructura Inmutable + Orquestaci√≥n.

---

## üöÄ Cat√°logo de Soluciones

<CardGrid>

  <LinkCard
    title="Stack LAMP: Ansible & Docker"
    description="Implementaci√≥n de referencia con gesti√≥n de secretos, construcci√≥n de im√°genes personalizadas y Makefiles."
    href="/es/projects/middleware/lamp-ansible-docker/"
  />

  <LinkCard
    title="WildFly: Orquestaci√≥n Inmutable"
    description="Despliegue automatizado de JBoss/WildFly. Reduce el tiempo de aprovisionamiento en un 98%."
    href="/es/projects/middleware/wildfly-ansible-docker/"
  />

  <LinkCard
    title="Roadmap: Alta Disponibilidad JBoss EAP"
    description="Dise√±o de arquitectura en cl√∫ster (Domain Mode) utilizando JGroups y TCPPING."
    href="#"
  />
</CardGrid>

---

## üõ†Ô∏è Stack Tecnol√≥gico

| Componente | Funci√≥n | Justificaci√≥n T√©cnica |
|:-----------|:--------|:----------------------|
| **Ansible** | Configuraci√≥n | Modelo sin agentes (Agentless) ideal para entornos con restricciones de seguridad estrictas. |
| **Docker** | Runtime | Aislamiento de procesos y portabilidad de binarios y dependencias. |
| **WildFly/JBoss** | Middleware | Est√°ndar de facto para aplicaciones Java EE en entorno Enterprise. |
| **Systemd** | Supervisi√≥n | Gesti√≥n nativa del ciclo de vida del servicio en el host Linux. |

---

## üéØ Competencias Clave

*   **Ingenier√≠a de Automatizaci√≥n:** Desarrollo de Playbooks idempotentes y roles reutilizables.
*   **Gesti√≥n de Secretos:** Desacoplamiento de credenciales y configuraci√≥n del c√≥digo fuente.
*   **Networking en Contenedores:** Exposici√≥n segura de servicios y gesti√≥n de interfaces de red.

:::note[Entorno de Validaci√≥n]
Las arquitecturas aqu√≠ descritas han sido validadas sobre la topolog√≠a de laboratorio LFCS, utilizando el nodo `docker-001` como host de ejecuci√≥n.
:::

```

---

## Contenido archivo: `src/content/docs/es/projects/middleware/wildfly-ansible-docker.mdx`

```bash
$ cat src/content/docs/es/projects/middleware/wildfly-ansible-docker.mdx
---
title: "Modernizaci√≥n Middleware: WildFly + Ansible"
description: Implementaci√≥n de una pipeline de Infraestructura como C√≥digo (IaC) para el despliegue inmutable de WildFly, eliminando la deriva de configuraci√≥n en entornos cr√≠ticos.
sidebar:
  label: Orquestaci√≥n WildFly
  order: 1
type: project
domain: automation
tags: ["ansible", "docker", "middleware", "devops", "hardening"]
---

import { Steps } from '@astrojs/starlight/components';

## üìã Resumen Ejecutivo

En infraestructuras cr√≠ticas (Banca y Telco), la gesti√≥n tradicional de servidores de aplicaciones basada en *scripts* manuales o "Pets" conlleva riesgos inaceptables: **deriva de configuraci√≥n**, **vulnerabilidades de seguridad** no parcheadas y un **Time-to-Market** lento para nuevas features.

Este proyecto implementa una estrategia de **Modernizaci√≥n de Infraestructura** migrando cargas de trabajo legacy de JBoss/WildFly hacia una arquitectura inmutable basada en contenedores.

### üéØ Objetivos de la Soluci√≥n
*   **Integridad Operacional:** Garantizar que el artefacto desplegado en Desarrollo es bit a bit id√©ntico al de Producci√≥n (Paridad de Entornos).
*   **Seguridad por Dise√±o:** Inyecci√≥n de secretos en tiempo de ejecuci√≥n, evitando credenciales hardcodeadas en archivos XML.
*   **Recuperaci√≥n ante Desastres (DR):** Reducci√≥n del RTO (Recovery Time Objective) de horas a segundos mediante el redespilegue automatizado.

---

## üèóÔ∏è Arquitectura y Decisiones de Dise√±o

La soluci√≥n desacopla el *runtime* (WildFly) de la configuraci√≥n, orquestado por un controlador centralizado.

```mermaid
graph LR
    subgraph Control_Plane [Plano de Control]
        A[Controlador Ansible]
    end

    subgraph Data_Plane [Plano de Datos]
        B[Host de Contenedores]
        C(Contenedor WildFly)
    end

    A -- SSH / Playbook IaC --> B
    B -- Spawns --> C
    C -- Puerto 8080 --> D[Tr√°fico App]
    C -- Puerto 9990 --> E[Tr√°fico Gesti√≥n]
    
    style Control_Plane fill:#f9f,stroke:#333,stroke-width:2px
    style Data_Plane fill:#ccf,stroke:#333,stroke-width:2px
```

### Principios T√©cnicos Aplicados
1.  **Inmutabilidad (Docker):** Se abandonan las actualizaciones "in-place" (yum update). Cada despliegue destruye el contenedor anterior y levanta una versi√≥n limpia y aprobada.
2.  **Idempotencia (Ansible):** El playbook puede ejecutarse N veces sin causar efectos secundarios adversos, garantizando el estado deseado.
3.  **12-Factor App:** La configuraci√≥n espec√≠fica del entorno (usuarios, contrase√±as, memoria JVM) se inyecta v√≠a **Variables de Entorno**, no editando el `standalone.xml`.

---

## üíª Implementaci√≥n T√©cnica

<Steps>

1.  **Definici√≥n del Inventario (IaC)**

    El inventario segrega los entornos, permitiendo aplicar pol√≠ticas diferentes a `staging` y `production` desde una √∫nica base de c√≥digo.

    ```ini title="inventory.ini"
    [middleware_nodes]
    # Nodo objetivo con rol de Host de Contenedores
    192.168.1.50 ansible_user=dzamo ansible_python_interpreter=/usr/bin/python3
    ```

2.  **Playbook de Aprovisionamiento**

    Este artefacto automatiza el ciclo de vida completo. N√≥tese el uso de `restart_policy: always` para resiliencia autom√°tica ante fallos del daemon.

    ```yaml title="deploy-wildfly.yml"
    ---
    - name: Orquestaci√≥n de Stack Middleware (WildFly)
      hosts: middleware_nodes
      become: true
      vars:
        # Versionado estricto para evitar actualizaciones accidentales
        wildfly_tag: "24.0.0.Final"
        container_name: "wildfly-prod-core"
        # En producci√≥n, estos valores vendr√≠an de Ansible Vault
        mgmt_user: "admin_ops"
        mgmt_pass: "{{ vault_mgmt_pass | default('SecurePass123!') }}"

      tasks:
        - name: Verificaci√≥n de Prerrequisitos (Python SDK)
          apt:
            name: python3-docker
            state: present
            update_cache: yes
            cache_valid_time: 3600

        - name: Pull de Imagen Base Aprobada
          community.docker.docker_image:
            name: "quay.io/wildfly/wildfly:{{ wildfly_tag }}"
            source: pull

        - name: Despliegue de Contenedor (Runtime)
          community.docker.docker_container:
            name: "{{ container_name }}"
            image: "quay.io/wildfly/wildfly:{{ wildfly_tag }}"
            state: started
            restart_policy: always
            ports:
              - "8080:8080" # Tr√°fico HTTP
              - "9990:9990" # Consola JBoss CLI
            # Binding a 0.0.0.0 es necesario en contenedores para exposici√≥n externa
            command: /opt/jboss/wildfly/bin/standalone.sh -b 0.0.0.0 -bmanagement 0.0.0.0
            env:
              WILDFLY_USERNAME: "{{ mgmt_user }}"
              WILDFLY_PASSWORD: "{{ mgmt_pass }}"
            labels:
              com.company.env: "production"
              com.company.service: "middleware-core"

        - name: Validaci√≥n de Salud del Servicio (Healthcheck)
          wait_for:
            host: "127.0.0.1"
            port: 9990
            delay: 5
            timeout: 60
    ```

3.  **Ejecuci√≥n y Verificaci√≥n**

    ```bash
    ansible-playbook -i inventory.ini deploy-wildfly.yml
    ```

</Steps>

## üîç An√°lisis de Valor e Impacto

La implementaci√≥n de este flujo automatizado reemplaza un procedimiento manual de **~40 pasos** documentados en Word/Excel, logrando:

| M√©trica | Antes (Manual) | Despu√©s (Automatizado) |
| :--- | :--- | :--- |
| **Tiempo de Aprovisionamiento** | 4 Horas | < 2 Minutos |
| **Error Humano** | Alto Riesgo | Eliminado (Idempotente) |
| **Escalabilidad** | Lineal (1 admin por N servidores) | Exponencial |

### Siguientes Pasos (Roadmap)
*   **Hardening:** Implementaci√≥n de escaneo de vulnerabilidades en la imagen (Trivy/Clair) en el pipeline CI.
*   **Alta Disponibilidad:** Evoluci√≥n hacia un Cl√∫ster JBoss en Modo Dominio utilizando **JGroups con TCPPING** para entornos sin multicast.

```

---

## Contenido archivo: `src/content/docs/es/projects/middleware/lamp-ansible-docker.mdx`

```bash
$ cat src/content/docs/es/projects/middleware/lamp-ansible-docker.mdx
---
title: "Stack LAMP con Ansible & Docker"
description: Despliegue automatizado de arquitectura web con gesti√≥n de secretos, construcci√≥n de im√°genes personalizadas y workflow estandarizado.
sidebar:
  label: Stack LAMP (Web)
  order: 2
type: project
domain: automation
tags: ["ansible", "docker", "lamp", "mysql", "security", "make"]
---

import { Steps } from '@astrojs/starlight/components';

## üìã Resumen Ejecutivo

Este proyecto moderniza el despliegue de aplicaciones PHP (Legacy), evolucionando de una configuraci√≥n manual a una **Infraestructura Inmutable**. A diferencia de los despliegues b√°sicos de contenedores, esta soluci√≥n aborda problemas del mundo real: la necesidad de compilar extensiones de PHP personalizadas (`mysqli`), la gesti√≥n segura de credenciales fuera del c√≥digo y la estandarizaci√≥n del entorno de desarrollo.

### üéØ Objetivos de Ingenier√≠a
1.  **Inmutabilidad:** Construcci√≥n de im√°genes Docker personalizadas (*custom builds*) en tiempo de despliegue para inyectar dependencias del sistema.
2.  **Seguridad (12-Factor App):** Desacoplamiento total de credenciales. El c√≥digo fuente no contiene secretos; estos se inyectan como variables de entorno.
3.  **Estandarizaci√≥n:** Implementaci√≥n de un `Makefile` como wrapper para abstraer la complejidad de Python/Ansible y facilitar el *onboarding*.

---

## üèóÔ∏è Arquitectura de la Soluci√≥n

El dise√±o implementa un patr√≥n de **Build & Deploy Remoto** orquestado por Ansible. El nodo de control prepara la configuraci√≥n y los secretos, mientras que el host remoto construye y ejecuta los contenedores.

```mermaid
graph LR
    %% --- Definici√≥n de Estilos ---
    classDef ansibleNode fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000000;
    classDef dockerNode fill:#e0f7fa,stroke:#0288d1,stroke-width:2px,color:#000000;
    classDef secretNode fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000000;

    subgraph Control_Node ["Nodo de Control (Dev)"]
        style Control_Node fill:#eceff1,stroke:#cfd8dc,color:#000000
        Make[("Makefile Wrapper")]:::ansibleNode
        Ansible[("Ansible Playbook")]:::ansibleNode
        Secrets[("secrets.yml<br/>(GitIgnored)")]:::secretNode
        
        Make --> Ansible
        Secrets -.-> Ansible
    end

    subgraph Remote_Host ["Servidor de Aplicaciones (Ubuntu 24.04)"]
        style Remote_Host fill:#0288d1,stroke:#cfd8dc,stroke-width:2px,color:#000000
        
        subgraph Docker_Runtime ["Docker Engine"]
            style Docker_Runtime fill:#eceff1,stroke:#455a64,color:#000000
            
            Build[("Docker Build<br/>(PHP + Mysqli)")]:::dockerNode
            App[("Container Web<br/>Apache")]:::dockerNode
            DB[("Container DB<br/>MariaDB")]:::dockerNode
            
            Ansible -- "1. Upload Dockerfile" --> Build
            Build -- "2. Image Created" --> App
            Ansible -- "3. Inject ENV Vars" --> App
            App -- "Internal DNS" --> DB
        end
    end
    
    linkStyle default stroke:#333,stroke-width:2px,fill:none;
```

---

## üíª Implementaci√≥n T√©cnica

La soluci√≥n utiliza una estructura profesional que separa configuraci√≥n, secretos y l√≥gica de despliegue.

<Steps>

1.  **Wrapper de Ejecuci√≥n (Makefile)**

    Para evitar problemas de versiones y entornos virtuales, encapsulamos los comandos en un `Makefile`. Esto garantiza que cualquier desarrollador use las mismas herramientas.

    ```makefile title="Makefile"
    setup:
        python3 -m venv .venv
        .venv/bin/pip install ansible-core docker
        .venv/bin/ansible-galaxy install -r requirements.yml

    deploy:
        .venv/bin/ansible-playbook playbook/deploy-lamp.yml
    ```

2.  **Gesti√≥n de Secretos (Patr√≥n de Sobreescritura)**

    Ansible carga primero las variables p√∫blicas (`all.yml`) y luego sobreescribe con un archivo de secretos local que est√° excluido de Git (`.gitignore`).

    ```yaml title="playbook/deploy-lamp.yml"
    - hosts: app_servers
      vars_files:
        - "../group_vars/all.yml"      # Estructura p√∫blica (Placeholders)
        - "../group_vars/secrets.yml"  # Secretos locales (Real Passwords)
    ```

3.  **L√≥gica del Playbook (Build & Run)**

    El playbook no solo descarga im√°genes, sino que sube un `Dockerfile` y compila una imagen personalizada para soportar `mysqli`.

    ```yaml title="playbook/deploy-lamp.yml"
    tasks:
      - name: Construir imagen PHP personalizada
        community.docker.docker_image:
          name: "{{ custom_image_name }}"
          tag: latest
          build:
            path: "{{ remote_project_path }}" # Ruta remota del Dockerfile
          source: build
          force_source: true

      - name: Desplegar Web Container
        community.docker.docker_container:
          name: "{{ project_name }}-web"
          image: "{{ custom_image_name }}:latest" # Usamos la imagen reci√©n creada
          env:
            # Inyecci√≥n segura de variables de entorno
            DB_HOST: "{{ project_name }}-db"
            DB_USER: "{{ mysql_user }}"
            DB_PASSWORD: "{{ mysql_pass }}"
    ```

4.  **C√≥digo Agn√≥stico (PHP)**

    El c√≥digo fuente (`src/index.php`) no contiene ninguna credencial. Lee la configuraci√≥n del entorno, cumpliendo con los principios de *The Twelve-Factor App*.

    ```php title="src/index.php"
    <?php
    // Lectura segura desde variables de entorno
    $servername = getenv('DB_HOST');
    $username = getenv('DB_USER');
    $password = getenv('DB_PASSWORD');

    $conn = new mysqli($servername, $username, $password, $dbname);
    
    if ($conn->connect_error) {
        die("‚ùå Fallo de conexi√≥n: " . $conn->connect_error);
    }
    echo "‚úÖ Conectado exitosamente a MariaDB.";
    ?>
    ```

5.  **Ejecuci√≥n Estandarizada**

    Gracias al Makefile, el despliegue se reduce a dos comandos, independientemente de la complejidad subyacente.

    ```bash
    # Inicializaci√≥n (Solo la primera vez)
    make setup

    # Despliegue idempotente
    make deploy
    ```

</Steps>

## üîç An√°lisis de Valor

| Caracter√≠stica | Script Bash Tradicional | Ansible + Docker Pro |
| :--- | :--- | :--- |
| **Credenciales** | Hardcoded en archivos (`.php`) | Inyectadas en Memoria (ENV) |
| **Dependencias** | Instalaci√≥n manual (`apt-get`) | Docker Build Autom√°tico |
| **Portabilidad** | "Funciona en mi m√°quina" | Estandarizado v√≠a Makefile |

### Pr√≥ximos Pasos (Roadmap AZ-104)

Este proyecto sirve como base para la migraci√≥n a la nube. Hemos documentado este proceso en la secci√≥n de Operaciones Cloud:

1. **Migraci√≥n IaaS:** Replicar esta arquitectura utilizando M√°quinas Virtuales en Azure.
    - **[üëâ Ver Escenario 01: Migraci√≥n LAMP a Azure IaaS](/es/cloud-ops/compute/scenario-01-lamp-iaas/)**
    
2. **Azure Container Registry (ACR):** Mover el proceso de *build* a la nube.
3. **Azure Key Vault:** Gesti√≥n de secretos centralizada.
```

---

## Contenido archivo: `src/content/docs/en/index.mdx`

```bash
$ cat src/content/docs/en/index.mdx
---
title: SysAdmin & DevOps
description: Infrastructure automation portfolio and architectural documentation by Daniel Zamo.
template: splash
hero:
  tagline: Infrastructure Orchestration, Enterprise Linux Administration, and Cloud Architecture.
  image:
    file: ../../../assets/images/hero-image.png
  actions:
    - text: View Featured Project (LAMP)
      link: /en/projects/middleware/lamp-ansible-docker/
      icon: rocket
      variant: primary
    - text: Browse Standard Procedures
      link: /en/linux-ops/
      icon: right-arrow
      variant: minimal
prev: false
next: false
editUrl: false
lastUpdated: 2025-12-06
head:
  - tag: meta
    attrs:
      property: og:title
      content: "Daniel Zamo | SysAdmin & DevOps"
  - tag: meta
    attrs:
      property: og:description
      content: "Infrastructure as Code (Ansible), Docker solutions, and Enterprise Linux Administration."
  - tag: meta
    attrs:
      property: og:image
      content: https://daniel-zamo.github.io/images/og-homepage-en-us.png
  - tag: meta
    attrs:
      property: og:image:width
      content: "1200"
  - tag: meta
    attrs:
      property: og:image:height
      content: "630"
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## Implementations & Core Competencies

<CardGrid stagger>
  <Card title="Automation & IaC" icon="rocket">
    Immutable infrastructure deployment. Featured case: **LAMP Stack** orchestration with secret management, custom Docker builds, and Makefiles.
    
    **[View Case Study: LAMP + Ansible ‚Üí](/en/projects/middleware/lamp-ansible-docker/)**
  </Card>

  <Card title="Azure Cloud Architecture" icon="microsoft">
    Hybrid solution design and identity governance. Aligned with **AZ-104** (Azure Administrator) standards.
    
    **[Explore Cloud Operations ‚Üí](/en/cloud-ops/)**
  </Card>
  
  <Card title="Enterprise Linux Operations" icon="linux">
    Technical knowledge base for mission-critical environments: LVM Storage Management, Security Hardening, and Troubleshooting.
    
    **[View Standard Procedures (SOP) ‚Üí](/en/linux-ops/)**
  </Card>
  
  <Card title="KVM/Libvirt Orchestration" icon="laptop">
    Low-level compute resource management. CLI-based Virtual Machine (VM) provisioning and performance optimization.
    
    **[View Virtualization Scenario ‚Üí](/en/linux-ops/scenarios/04-kvm-management/)**
  </Card>

  <Card title="Azure Cloud Architecture" icon="microsoft">
    Hybrid solution design and identity governance. Aligned with **AZ-104** (Azure Administrator) standards.
    
    *(Status: Active Certification Roadmap)*
  </Card>
</CardGrid>

```

---

## Contenido archivo: `src/content/docs/en/linux-ops/index.mdx`

```bash
$ cat src/content/docs/en/linux-ops/index.mdx
---
title: Linux Systems Operations (Enterprise)
description: Technical knowledge base for administration, hardening, and optimization of Linux systems.
sidebar:
  label: Overview
  order: 0
type: theory
source: manual
---

import { Card, CardGrid, LinkButton, Icon } from '@astrojs/starlight/components';

This repository consolidates **Standard Operating Procedures (SOP)** and engineering best practices for Linux-based infrastructure management.

Content reflects production standards, prioritizing stability, security, and auditability in mission-critical environments.

:::tip[Operational Focus]
Documentation prioritizes **CLI (Command Line Interface)** over graphical tools, ensuring management capabilities in *headless* servers and task automation.
:::

## üéØ Competency Domains

Functional areas critical for service continuity:

<CardGrid>
  <Card title="Core Operations" icon="linux">
    Text stream manipulation, process management, and Bash scripting.
  </Card>
  <Card title="Runtime Management" icon="setting">
    Boot analysis (Systemd), kernel parameters, and system tuning.
  </Card>
  <Card title="Identity Management" icon="user">
    Local users, LDAP/AD integration, and PAM policies.
  </Card>
  <Card title="Networking & Security" icon="signal">
    Routing, Firewalling, and precise NTP/PTP synchronization.
  </Card>
  <Card title="Infrastructure Services" icon="puzzle">
    DNS, Secure SSH, Web Servers, and Container Runtimes.
  </Card>
  <Card title="Enterprise Storage" icon="laptop">
    LVM Management, software RAID, and transactional filesystems.
  </Card>
</CardGrid>

---

## üß™ Standard Procedures

Access the scenario catalog to view step-by-step implementations of administration tasks.

<div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 1rem;">
    <LinkButton href="/en/linux-ops/scenarios/" variant="primary" icon="rocket">
        View Procedure Catalog
    </LinkButton>
</div>
```

---

## Contenido archivo: `src/content/docs/en/linux-ops/scenarios/index.mdx`

```bash
$ cat src/content/docs/en/linux-ops/scenarios/index.mdx
---
title: Operational Scenarios Catalog
description: Technical procedures for incident resolution, resource provisioning, and Linux lifecycle management.
sidebar:
  label: Scenario Catalog
  order: 0
type: scenario
source: manual
---

import { CardGrid, LinkCard, Icon } from '@astrojs/starlight/components';

This section hosts the **practical execution layer** of the portfolio. Each entry is structured as a **Standard Operating Procedure (SOP)**, simulating real-world infrastructure tickets requiring configuration, troubleshooting, and validation in a live environment.

The objective is to demonstrate resolution capabilities under technical constraints aligned with industry standards (LFCS/RHCSA).

---

## üèóÔ∏è Environment Topology (Standard Operating Environment)

All scenarios run on a standardized 7-node cluster. This topology simulates a three-tier architecture (Web, App, Data) with nested virtualization capabilities.

:::danger[Access & Security Protocols]
*   **Bastion:** All operations originate from the `terminal` node.
*   **Nested SSH:** Strictly forbidden by security policy. Connections must be direct from the controller.
*   **Persistence:** All configurations must survive a system reboot (*Reboot-proof*).
:::

### Node Inventory

| Role | Hostname | Function / Business Context |
| :--- | :--- | :--- |
| **Management** | `terminal` | Operations Controller (Bastion Host) and main CLI. |
| **Web Tier** | `web-srv1` | SSL Termination, Reverse Proxy, and HTTP Server. |
| **App Tier** | `app-srv1` | Business logic execution and internal APIs. |
| **Data Tier** | `data-001/002` | Persistence, databases, and raw block storage. |
| **Virtualization** | `kvm-001` | Libvirt hypervisor for server consolidation. |
| **Containers** | `docker-001` | Runtime for microservices and lightweight orchestration. |

---

## üöÄ Available Procedures

Select a scenario to view engineering requirements and implementation strategy.

<CardGrid>
  <LinkCard
    title="01 - Time Management (NTP/Chrony)"
    description="Configuration of time clients to ensure transactional consistency in distributed systems."
    href="/en/linux-ops/scenarios/01-time-sync/"
  />
  <LinkCard
    title="02 - L7 Load Balancing (Nginx)"
    description="Implementation of Reverse Proxy and HTTP traffic balancing for internal applications."
    href="/en/linux-ops/scenarios/02-nginx-proxy/"
  />
  <LinkCard
    title="03 - Storage & LVM"
    description="Elastic volume management: Hot resizing and Physical Volume lifecycle management."
    href="/en/linux-ops/scenarios/03-storage-mgmt/"
  />
   <LinkCard
    title="04 - KVM Orchestration"
    description="Provisioning of virtual machines (VMs) and lifecycle management using Virsh."
    href="/en/linux-ops/scenarios/04-kvm-management/"
  />
</CardGrid>

:::tip[Infrastructure Note]
Procedures assume pre-configured network connectivity between inventory nodes. For specific IP addressing details, refer to the technical datasheet within each scenario.
:::

```

---

## Contenido archivo: `src/content/docs/en/linux-ops/scenarios/04-kvm-management.mdx`

```bash
$ cat src/content/docs/en/linux-ops/scenarios/04-kvm-management.mdx
---
title: "KVM/Libvirt VM Orchestration"
description: Provisioning and lifecycle management of compute instances using virtualization CLI tools.
sidebar:
  label: KVM Orchestration
  order: 4
type: scenario
domain: virtualization
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Engineering Requirements

**Business Context:**
The QA team requests a dedicated, isolated compute instance on the `kvm-001` hypervisor for integration testing. The environment must be provisioned using standard CLI tools to ensure reproducibility.

**Technical Objectives:**
1.  **Host Readiness:** Validate the `qemu-kvm` and `libvirtd` stack status.
2.  **Storage:** Provision a **10 GiB** `qcow2` volume (Thin Provisioned) named `qa-vm-disk.qcow2`.
3.  **Instance Definition (`qa-vm01`):**
    *   **Specs:** 1 vCPU, 1024 MiB RAM.
    *   **Network:** Default NAT bridge.
    *   **Console:** Serial redirection (Headless mode).
4.  **Lifecycle:** Enable auto-start on boot and perform a graceful shutdown test.

:::note[Scope Constraint]
Full OS installation is not required. Successfully booting the kernel into the installer prompt via the serial console is sufficient for validation.
:::

---

## üîß Implementation Procedure

<Steps>

1.  **Access Hypervisor Node**

    Connect to the virtualization host.

    ```bash
    ssh kvm-001
    ```

2.  **Verify Host Prerequisites**

    Ensure the libvirt daemon is active and enabled.

    ```bash
    sudo systemctl is-active libvirtd
    # If inactive: sudo systemctl enable --now libvirtd
    ```

3.  **Provision Storage Volume**

    Create the virtual disk using `qemu-img`.

    ```bash
    sudo qemu-img create -f qcow2 /var/lib/libvirt/images/qa-vm-disk.qcow2 10G
    ```

4.  **Define and Boot Instance**

    Execute `virt-install` with parameters for a headless environment (`--graphics none`).

    ```bash
    sudo virt-install \
      --name qa-vm01 \
      --ram 1024 \
      --vcpus 1 \
      --disk path=/var/lib/libvirt/images/qa-vm-disk.qcow2,format=qcow2 \
      --os-variant rocky9 \
      --network network=default \
      --graphics none \
      --console pty,target_type=serial \
      --location "https://download.rockylinux.org/pub/rocky/9/BaseOS/x86_64/os/" \
      --extra-args "console=ttyS0"
    ```

    *Tip: To detach from the console without killing the VM, press `Ctrl + ]`.*

5.  **Configure Autostart**

    Ensure the VM survives host reboots.

    ```bash
    sudo virsh autostart qa-vm01
    ```

6.  **Validate Lifecycle Control**

    Verify the VM is running, then request an ACPI shutdown.

    ```bash
    # Check status
    sudo virsh list --all

    # Graceful shutdown request
    sudo virsh shutdown qa-vm01
    ```

</Steps>

## üîç Troubleshooting

*   **"Guest name already in use":** If a previous attempt failed partially, clean up the metadata:
    ```bash
    sudo virsh destroy qa-vm01
    sudo virsh undefine qa-vm01
    ```
*   **Console Hanging:** If the console does not show output, ensure `--extra-args "console=ttyS0"` was passed correctly to redirect kernel logs to the serial port.

```

---

## Contenido archivo: `src/content/docs/en/linux-ops/scenarios/01-time-sync.mdx`

```bash
$ cat src/content/docs/en/linux-ops/scenarios/01-time-sync.mdx
---
title: "NTP Strategy & Time Synchronization"
description: Configuration of systemd-timesyncd policies for transactional consistency in distributed clusters.
sidebar:
  label: NTP Strategy
  order: 1
type: scenario
domain: networking
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Engineering Requirements

**Business Context:**
Audit logs indicate drift in system clocks across the cluster, causing failures in distributed transaction reconciliation. A strict NTP policy must be enforced on the controller node to ensure consistency.

**Technical Objectives:**
1.  **Primary Upstreams:** Configure `0.pool.ntp.org` and `1.pool.ntp.org` as authoritative sources.
2.  **Failover:** Define `ntp.ubuntu.com` and `0.debian.pool.ntp.org` as fallback servers.
3.  **Performance Tuning:**
    *   Max poll interval: `1000s`.
    *   Connection retry interval: `20s`.

:::caution[Service Constraint]
The environment utilizes `systemd-timesyncd` as the standard lightweight SNTP client. Do not install `chrony` or legacy `ntp` packages unless explicitly required, to avoid service conflicts.
:::

---

## üîß Implementation Procedure

<Steps>

1.  **Audit Current State**

    Verify the operating system version and the active time synchronization status before applying changes.

    ```bash
    # Check OS version
    cat /etc/os-release | grep PRETTY_NAME

    # Verify active time service status
    timedatectl timesync-status
    ```

2.  **Apply Configuration Policy**

    Edit the daemon configuration file `/etc/systemd/timesyncd.conf`.

    ```bash
    sudo vim /etc/systemd/timesyncd.conf
    ```

    Modify the `[Time]` section to strictly match the engineering requirements:

    ```ini title="/etc/systemd/timesyncd.conf" {2-5}
    [Time]
    NTP=0.pool.ntp.org 1.pool.ntp.org
    FallbackNTP=ntp.ubuntu.com 0.debian.pool.ntp.org
    PollIntervalMaxSec=1000
    ConnectionRetrySec=20
    ```

3.  **Restart Service**

    Apply the new policy by restarting the systemd unit.

    ```bash
    sudo systemctl restart systemd-timesyncd.service
    ```

4.  **Validate Implementation**

    Confirm the service is active and synchronizing with the correct upstream stratum.

    ```bash
    # Check service status
    sudo systemctl status systemd-timesyncd.service

    # Validate metrics (Poll interval)
    timedatectl timesync-status
    ```

    *Validation Criteria:* Output must show `Active: active (running)` and `Poll interval` metrics matching the configuration.

</Steps>

## üîç Troubleshooting

If synchronization fails (e.g., "Packet not received"), inspect the system journal:

```bash
sudo journalctl -u systemd-timesyncd --no-pager | tail -n 20
```

*Success Indicator:* Look for logs stating: *"Contacted time server ... (0.pool.ntp.org)"*.

```

---

## Contenido archivo: `src/content/docs/en/linux-ops/scenarios/05-kvm-networking.mdx`

```bash
$ cat src/content/docs/en/linux-ops/scenarios/05-kvm-networking.mdx
---
title: "Advanced KVM Networking: Bridges & VLANs"
description: Design and implementation of 802.1Q network segmentation and persistent Linux Bridges on a virtualization host.
sidebar:
  label: KVM Networking (VLANs)
  order: 5
type: scenario
domain: networking
---

import { Steps, LinkButton } from '@astrojs/starlight/components';

## üìã Architectural Definition

**Engineering Context:**
In bare-metal virtualization environments (KVM Host), utilizing a "Flat Network" introduces security risks and broadcast congestion. It is necessary to evolve the host network configuration to support **multiple isolation zones** using a single physical interface (Trunk).

**Design Objectives:**
1.  **L3 Decoupling:** The host management IP must not reside on the physical interface but on a virtual Bridge, enabling greater flexibility.
2.  **802.1Q Segmentation:** Implementation of VLAN sub-interfaces to segregate Management traffic (VLAN 19) from Lab/Tenant traffic (VLAN 88).
3.  **NetworkManager Persistence:** Declarative configuration via `nmcli` to ensure stability across reboots, deprecating legacy scripts.

---

## üèóÔ∏è Host Logical Topology

The following diagram details the internal network stack configured within the KVM Host (`kvm-001`).

```mermaid
graph TD
    subgraph Physical_Layer [Physical Layer]
        ENO1[("Physical Interface<br/>(eno1 - Trunk)")]:::phys
    end

    subgraph VLAN_Layer [802.1Q Layer - Tagging]
        VLAN19["eno1.19<br/>(VLAN ID: 19)"]:::vlan
        VLAN88["eno1.88<br/>(VLAN ID: 88)"]:::vlan
    end

    subgraph Bridge_Layer [L2 Layer - Virtual Switching]
        BR19["br19<br/>(Mgmt Bridge)"]:::bridge
        BR88["br88<br/>(Workload Bridge)"]:::bridge
    end

    subgraph IP_Layer [L3 Layer - Routing]
        IP_HOST["KVM Host IP<br/>192.168.19.50/24"]:::ip
        VM_MGMT["Controller VM"]:::vm
        VM_LAB["App VM"]:::vm
    end

    ENO1 ==> VLAN19
    ENO1 ==> VLAN88
    
    VLAN19 --> BR19
    VLAN88 --> BR88
    
    BR19 -.-> IP_HOST
    BR19 --> VM_MGMT
    BR88 --> VM_LAB

    %% Professional Styles
    classDef phys fill:#e3f2fd,stroke:#1565c0,color:#0d47a1,stroke-width:2px;
    classDef vlan fill:#fff9c4,stroke:#fbc02d,color:#000;
    classDef bridge fill:#c8e6c9,stroke:#2e7d32,color:#1b5e20,stroke-width:2px;
    classDef ip fill:#ffccbc,stroke:#d84315,color:#bf360c;
    classDef vm fill:#f5f5f5,stroke:#616161,color:#000;
```

---

## üöÄ Implementation Procedure (SOP)

This design has been standardized and documented in the **Ops Standards Library**. The SOP details the atomic `nmcli` commands required to transform a standard interface into a VLAN trunk with bridges.

<div style="display: flex; justify-content: center; gap: 1rem; margin: 2rem 0;">
  <LinkButton 
    href="https://github.com/daniel-zamo/ops-standards-library/blob/main/linux/SOP-LINUX-NM-01_bridge-vlan-config.md"
    variant="primary"
    icon="github"
    target="_blank"
  >
    View Technical Procedure (GitHub)
  </LinkButton>
  <LinkButton 
    href="https://github.com/daniel-zamo/ops-standards-library/tree/main/linux"
    variant="secondary"
    icon="external"
    target="_blank"
  >
    Explore OPS Repo
  </LinkButton>
</div>

### Configuration Outcomes

Upon applying this architecture:

1.  **Isolation:** VMs connected to `br88` are logically isolated from the host management network.
2.  **Standardization:** Dependency on unpredictable interface names (e.g., `eth0` vs `enp3s0`) is eliminated by abstracting connectivity into standardized Bridges (`brXX`).
3.  **Observability:** The structure allows for independent traffic inspection of each segment using `tcpdump -i br19`.

```

---

## Contenido archivo: `src/content/docs/en/linux-ops/scenarios/03-storage-mgmt.mdx`

```bash
$ cat src/content/docs/en/linux-ops/scenarios/03-storage-mgmt.mdx
---
title: "LVM Volume Lifecycle Management"
description: Dynamic resizing of Volume Groups and decommissioning of Physical Volumes in production.
sidebar:
  label: LVM Management
  order: 3
type: scenario
domain: storage
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Engineering Requirements

**Business Context:**
A storage optimization initiative requires decommissioning the physical disk `/dev/vdh` from the existing volume group `vol1`. This disk must be repurposed to create a new isolated storage group (`vol2`) for a specific project.

**Technical Objectives:**
1.  **Hot Migration:** Evacuate data from `/dev/vdh` without service interruption using LVM mirroring capabilities.
2.  **Topology Change:** Remove `/dev/vdh` from `vol1` and initialize a new Volume Group `vol2`.
3.  **Provisioning:** Create a Logical Volume `p1` (50 MiB) within `vol2`.
4.  **Filesystem:** Format the new volume with **ext4**.

:::caution[Data Integrity]
In production, executing `pvmove` is an I/O intensive operation. Ensure no critical backups are running simultaneously.
:::

---

## üîß Implementation Procedure

<Steps>

1.  **Analyze Storage Topology**

    Confirm the current layout and verify `/dev/vdh` is part of `vol1`.

    ```bash
    sudo pvs
    sudo vgs
    ```

2.  **Data Evacuation (pvmove)**

    Relocate allocated physical extents (PE) from the target disk to other free disks within the same Volume Group.

    ```bash
    # Verify allocation
    sudo pvdisplay -m /dev/vdh

    # Migrate data
    sudo pvmove /dev/vdh
    ```

3.  **Decommission Disk**

    Once the disk is fully free, remove it from the Volume Group.

    ```bash
    sudo vgreduce vol1 /dev/vdh
    ```

4.  **Initialize New Storage Group**

    Create the new Volume Group `vol2` using the reclaimed physical volume.

    ```bash
    sudo vgcreate vol2 /dev/vdh
    ```

5.  **Provision Logical Volume**

    Allocate the requested storage capacity.

    ```bash
    sudo lvcreate -n p1 -L 50M vol2
    ```

6.  **Apply Filesystem**

    Format the block device.

    ```bash
    sudo mkfs.ext4 /dev/vol2/p1
    ```

7.  **Final Validation**

    Verify the new LVM structure and filesystem signature.

    ```bash
    sudo lvs -o lv_name,vg_name,lv_size,seg_pe_ranges
    lsblk -f /dev/vol2/p1
    ```

</Steps>

## üîç Troubleshooting

*   **"Physical volume is still in use":** The `vgreduce` command will fail if you skipped Step 2. You **must** evacuate data first.
*   **"Insufficient free extents":** If `vol1` does not have enough free space on other disks (e.g., `/dev/vdb`) to hold the data from `/dev/vdh`, the `pvmove` operation will fail.

```

---

## Contenido archivo: `src/content/docs/en/linux-ops/scenarios/02-nginx-proxy.mdx`

```bash
$ cat src/content/docs/en/linux-ops/scenarios/02-nginx-proxy.mdx
---
title: "L7 Load Balancing & Edge Proxy"
description: Implementation of Reverse Proxy rules and traffic distribution algorithms using Nginx.
sidebar:
  label: L7 Load Balancing
  order: 2
type: scenario
domain: networking
source: manual
---

import { Steps } from '@astrojs/starlight/components';
import LabEnvironment from '../../../../../components/LabEnvironment.astro';

<LabEnvironment />

## üìã Engineering Requirements

**Business Context:**
Legacy internal applications hosted on `web-server01` are currently exposed on non-standard ports (`1111`, `2222`), complicating firewall rules and client access. A Reverse Proxy solution is required to standardize access via HTTP.

**Technical Objectives:**
1.  **Target Node:** `web-server01` (10.0.0.60).
2.  **Traffic Redirection (Port 8001):**
    *   Incoming traffic must be forwarded to the specific backend path: `http://127.0.0.1:2222/special`.
3.  **Load Balancing (Port 8000):**
    *   Distribute traffic between **App A** (`:1111`) and **App B** (`:2222`).
    *   Algorithm: Round Robin (Default) or Random.

:::danger[Operational Constraints]
*   **Immutable Backend:** Do not modify the existing Nginx configuration files serving ports 1111/2222.
*   **Persistence:** The configuration must persist after a system reboot.
:::

---

## üîß Implementation Procedure

<Steps>

1.  **Access Target Node**

    Establish a secure connection to the web server from the bastion host.

    ```bash
    ssh web-server01
    ```

2.  **Verify Backend Health**

    Ensure the upstream applications are operational before configuring the proxy.

    ```bash
    curl -I http://127.0.0.1:1111
    curl -I http://127.0.0.1:2222
    ```

3.  **Define Proxy Configuration**

    Create a modular configuration file in `/etc/nginx/conf.d/`. Avoid editing the main `nginx.conf` directly.

    ```bash
    sudo vim /etc/nginx/conf.d/loadbalancer.conf
    ```

    Implement the following block to define the upstream group and server contexts:

    ```nginx title="/etc/nginx/conf.d/loadbalancer.conf"
    # 1. Define Upstream Group (Load Balancer)
    upstream backend_apps {
        server 127.0.0.1:1111;
        server 127.0.0.1:2222;
    }

    # 2. Task: Load Balancer (Port 8000)
    server {
        listen 8000;
        server_name localhost;

        location / {
            proxy_pass http://backend_apps;
        }
    }

    # 3. Task: Redirection/Proxy (Port 8001)
    server {
        listen 8001;
        server_name localhost;

        location / {
            proxy_pass http://127.0.0.1:2222/special;
        }
    }
    ```

4.  **Validate and Reload**

    Perform a syntax check to prevent service downtime, then reload the process signal.

    ```bash
    sudo nginx -t
    sudo systemctl reload nginx
    ```

5.  **Functional Verification**

    **Test Load Balancer (8000):** Run multiple requests. Responses should alternate between backends.
    ```bash
    for i in {1..4}; do curl http://localhost:8000; echo; done
    ```

    **Test Redirection (8001):**
    ```bash
    curl -v http://localhost:8001
    ```

</Steps>

## üîç Troubleshooting

*   **Connection Refused:** Ensure Nginx is bound to the new ports:
    ```bash
    ss -tulpn | grep nginx
    ```
*   **502 Bad Gateway:** Indicates Nginx cannot reach the upstream ports (1111/2222). Check if the backend apps crashed.

```

---

## Contenido archivo: `src/content/docs/en/cloud-ops/index.mdx`

```bash
$ cat src/content/docs/en/cloud-ops/index.mdx
---
title: Azure Architecture & Operations
description: Technical documentation for the design, governance, and administration of Microsoft Azure Cloud environments.
sidebar:
  label: Overview
  order: 0
type: theory
domain: cloud
---

import { Card, CardGrid, LinkButton } from '@astrojs/starlight/components';

This section consolidates **engineering procedures** and reference architectures for cloud infrastructure implementation.

The content aligns with the best practices of the **Microsoft Cloud Adoption Framework (CAF)**, prioritizing security, automation, and operational governance over manual management.

---

## ‚òÅÔ∏è Architectural Domains

<CardGrid>
  <Card title="Identity & Governance" icon="user">
    Implementation of access policies (RBAC), identity management with Entra ID, and compliance via Azure Policy.
  </Card>
  <Card title="Networking (VNETs)" icon="signal">
    Design of Hub-and-Spoke topologies, traffic segmentation with NSGs, and secure hybrid connectivity.
  </Card>
  <Card title="Compute IaaS" icon="laptop">
    Virtual Machine lifecycle management, cost optimization (Spot Instances), and auto-scaling.
  </Card>
  <Card title="Storage & Data" icon="document">
    Management of storage accounts, data protection, and redundancy strategies (LRS/GRS).
  </Card>
</CardGrid>

---

## üöÄ Technical Implementations

Practical deployment scenarios and infrastructure troubleshooting.

<div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 1rem;">
    <LinkButton href="/en/cloud-ops/compute/scenario-01-lamp-iaas/" variant="primary" icon="rocket">
        View Scenario 01: LAMP Stack IaaS
    </LinkButton>
</div>

```

---

## Contenido archivo: `src/content/docs/en/cloud-ops/compute/scenario-01-lamp-iaas.mdx`

```bash
$ cat src/content/docs/en/cloud-ops/compute/scenario-01-lamp-iaas.mdx
---
title: "Scenario 01: LAMP Migration to Azure IaaS"
description: Automated deployment of a Linux web server (Apache/PHP/MySQL) on Azure using CLI and Cloud-Init.
sidebar:
  label: 01 - LAMP Stack IaaS
  order: 1
type: scenario
domain: compute
---

import { Steps } from '@astrojs/starlight/components';

## üìã Project Definition

**Business Context:**
As part of a modernization initiative, a legacy web workload (LAMP Stack) needs to be migrated to the public cloud. The objective is to reduce the operational overhead of physical hardware while maintaining full control over the operating system configuration.

**Technical Requirements:**
1.  **Resource Isolation:** Implementation of a dedicated Resource Group for lifecycle management.
2.  **Network Security:** Segmentation via VNET and strict traffic filtering (NSG) allowing only HTTP (80) and SSH (22).
3.  **Compute Efficiency:** Provisioning of a `Standard_B1s` (Burstable) instance on Ubuntu 22.04 LTS.
4.  **Automation (Infrastructure as Code):** Provisioning must be unattended, utilizing **Cloud-Init** for software stack configuration upon first boot.

:::tip[Provisioning Strategy]
In production environments, manual post-deployment configuration is avoided. Using **Custom Data (Cloud-Init)** ensures the VM is operational and configured from second zero, facilitating horizontal scalability.
:::

---

## üîß Implementation Procedure (Azure CLI)

Execution via `az cli` to ensure deployment reproducibility and auditability.

<Steps>

1.  **Environment Variable Definition**

    Standardizing naming conventions for resources.

    ```bash
    RG_NAME="rg-lamp-migration-001"
    LOCATION="northeurope"
    VM_NAME="vm-web-legacy-01"
    VNET_NAME="vnet-prod-01"
    NSG_NAME="nsg-web-public"
    ```

2.  **Logical Container Creation**

    ```bash
    az group create --name $RG_NAME --location $LOCATION
    ```

3.  **Bootstrapping Configuration (Cloud-Init)**

    Defining the desired state of the operating system. This manifest installs dependencies and generates test files automatically.

    ```yaml title="cloud-init-lamp.txt"
    #cloud-config
    package_upgrade: true
    packages:
      - apache2
      - php
      - libapache2-mod-php
      - mysql-server
      - php-mysql
    write_files:
      - content: |
          <?php phpinfo(); ?>
        path: /var/www/html/info.php
    runcmd:
      - systemctl restart apache2
      - systemctl enable apache2
    ```

4.  **Infrastructure Deployment**

    Atomic command that provisions network, security, public IP, and compute, injecting the Cloud-Init configuration.

    ```bash
    az vm create \
      --resource-group $RG_NAME \
      --name $VM_NAME \
      --image Ubuntu2204 \
      --admin-username azureuser \
      --generate-ssh-keys \
      --public-ip-sku Standard \
      --size Standard_B1s \
      --custom-data @cloud-init-lamp.txt \
      --nsg $NSG_NAME
    ```

5.  **Network Hardening (NSG)**

    Explicit configuration of inbound rules to allow web traffic.

    ```bash
    az network nsg rule create \
      --resource-group $RG_NAME \
      --nsg-name $NSG_NAME \
      --name Allow-HTTP \
      --protocol Tcp \
      --priority 100 \
      --destination-port-range 80 \
      --access Allow
    ```

6.  **Service Verification**

    Validation of the public endpoint availability.

    ```bash
    IP_ADDRESS=$(az vm show -d -g $RG_NAME -n $VM_NAME --query publicIps -o tsv)
    echo "Endpoint available at: http://$IP_ADDRESS/info.php"
    ```

</Steps>

## üîç Design Decision

### Why Cloud-Init for this scenario?
For **single instance** or **base image** deployments, Cloud-Init is the "Cloud-Native" industry standard. It removes the need for an external configuration server (like Ansible Tower) for the initial bootstrap, reducing complexity and Recovery Time Objective (RTO).

```

---

## Contenido archivo: `src/content/docs/en/projects/index.mdx`

```bash
$ cat src/content/docs/en/projects/index.mdx
---
title: Engineering Portfolio
description: Infrastructure automation initiatives, DevOps practices, and Cloud-Native architectures.
sidebar:
  label: Overview
  order: 0
type: project
---

import { Card, CardGrid, LinkButton } from '@astrojs/starlight/components';

This section consolidates **production-grade projects** demonstrating practical expertise in solving complex infrastructure challenges.

Each entry is documented not just at the code level, but also justifying **architectural decisions** and **business impact**.

---

## üéØ Specialization Domains

<CardGrid>
  <Card title="Middleware Automation" icon="puzzle">
    Orchestration of enterprise application servers and technical debt reduction using Infrastructure as Code (IaC).
    
    **Stack:** Ansible, Docker, JBoss/WildFly.
    
    [View Middleware Projects ‚Üí](/en/projects/middleware/)
  </Card>
  
  <Card title="Cloud Architecture & Governance" icon="rocket">
    **Active Roadmap:** Design of Azure Landing Zones, Identity Management (IAM), and hybrid networking. Reference implementation aligned with **AZ-104** certification.
    
    **Stack:** Azure Resource Manager, VNet, Entra ID.
  </Card>
  
  <Card title="Compute & Virtualization" icon="laptop">
    **Active Roadmap:** KVM/Libvirt hypervisor automation and Virtual Machine (VM) provisioning strategies.
    
    **Stack:** QEMU/KVM, Virsh, Cloud-Init.
  </Card>
  
  <Card title="Release Engineering (CI/CD)" icon="setting">
    **Active Roadmap:** Continuous integration pipelines, artifact management, and automated deployment strategies.
    
    **Stack:** GitLab CI, Container Registries.
  </Card>
</CardGrid>

---

## üìä Transversal Skills

These projects validate competencies beyond command execution:

- **Infrastructure as Code (IaC):** Declarative resource management ensuring consistency.
- **Immutability:** Adoption of container patterns to eliminate configuration drift.
- **Modernization:** Ability to transform legacy systems into manageable architectures.
- **Technical Documentation:** Effective communication of complex designs for operations teams.

---

## üöÄ Quick Navigation

<div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 1.5rem;">
  <LinkButton href="/en/projects/middleware/" variant="primary" icon="right-arrow">
    Middleware Solutions
  </LinkButton>
  <LinkButton href="/en/linux-ops/" variant="secondary" icon="document">
    Explore Linux Operations
  </LinkButton>
</div>

:::tip[Professional Context]
The scenarios presented reflect real-world challenges encountered in corporate environments (Banking, Telco), prioritizing **security**, **maintainability**, and **operational excellence**.
:::

```

---

## Contenido archivo: `src/content/docs/en/projects/middleware/index.mdx`

```bash
$ cat src/content/docs/en/projects/middleware/index.mdx
---
title: Middleware Engineering & Automation
description: Modernization strategies for enterprise application servers using IaC and containerization.
sidebar:
  label: Middleware
  order: 0
type: project
---

import { CardGrid, LinkCard } from '@astrojs/starlight/components';

## Architectural Vision

This section documents the **modernization of middleware platforms**. The goal is to transition monolithic, manual infrastructures toward elastic, code-managed architectures.

### The Business Problem

Traditional management of application servers introduces critical friction:
*   **Operational Risk:** Manual configuration leads to human error and Environment Drift.
*   **Maintenance Cost:** Excessive time spent on manual patching (Toil).

**Solution:** Immutable Infrastructure + Orchestration.

---

## üöÄ Solution Catalog

<CardGrid>
  <LinkCard
    title="LAMP Stack: Ansible & Docker"
    description="Reference implementation featuring secret management, custom image builds, and standardized Makefiles."
    href="/en/projects/middleware/lamp-ansible-docker/"
  />

  <LinkCard
    title="WildFly: Immutable Orchestration"
    description="Automated deployment of JBoss/WildFly. Reduces provisioning time by 98%."
    href="/en/projects/middleware/wildfly-ansible-docker/"
  />  
  
  <LinkCard
    title="Roadmap: JBoss EAP High Availability"
    description="Cluster architecture design (Domain Mode) utilizing JGroups and TCPPING."
    href="#"
  />
</CardGrid>

---

## üõ†Ô∏è Tech Stack

| Component | Function | Technical Justification |
|:-----------|:--------|:----------------------|
| **Ansible** | Configuration | Agentless model ideal for environments with strict security constraints. |
| **Docker** | Runtime | Process isolation and portability of binaries and dependencies. |
| **WildFly/JBoss** | Middleware | De facto standard for Java EE applications in Enterprise environments. |
| **Systemd** | Supervision | Native service lifecycle management on the Linux host. |

---

## üéØ Key Competencies

*   **Automation Engineering:** Development of idempotent Playbooks and reusable roles.
*   **Secret Management:** Decoupling credentials and configuration from source code.
*   **Container Networking:** Secure service exposure and network interface management.

:::note[Validation Environment]
The architectures described here have been validated using the LFCS lab topology, utilizing the `docker-001` node as the execution host.
:::

```

---

## Contenido archivo: `src/content/docs/en/projects/middleware/wildfly-ansible-docker.mdx`

```bash
$ cat src/content/docs/en/projects/middleware/wildfly-ansible-docker.mdx
---
title: "Middleware Modernization: WildFly + Ansible"
description: Implementation of an Infrastructure as Code (IaC) pipeline for immutable WildFly deployments, eliminating configuration drift in critical environments.
sidebar:
  label: WildFly Orchestration
  order: 1
type: project
domain: automation
tags: ["ansible", "docker", "middleware", "devops", "hardening"]
---

import { Steps } from '@astrojs/starlight/components';

## üìã Executive Summary

In critical infrastructure sectors (Banking & Telco), traditional "Pet-based" middleware management leads to unacceptable risks: **configuration drift**, unpatched **security vulnerabilities**, and slow **Time-to-Market** for new features.

This project implements an **Infrastructure Modernization** strategy, migrating legacy JBoss/WildFly workloads toward an immutable, container-based architecture.

### üéØ Solution Objectives
*   **Operational Integrity:** Ensure the artifact deployed in Development is bit-for-bit identical to Production (Environment Parity).
*   **Security by Design:** Runtime secret injection, avoiding hardcoded credentials in XML configuration files.
*   **Disaster Recovery (DR):** Reduction of RTO (Recovery Time Objective) from hours to seconds via automated redeployment.

---

## üèóÔ∏è Architecture & Design Decisions

The solution decouples the runtime (WildFly) from configuration, orchestrated by a centralized controller.

```mermaid
graph LR
    subgraph Control_Plane [Control Plane]
        A[Ansible Controller]
    end
    
    subgraph Data_Plane [Data Plane]
        B[Container Host]
        C(Immutable WildFly)
    end

    A -- SSH / IaC Playbook --> B
    B -- Spawns --> C
    C -- Exposes :8080 --> D[App Traffic]
    C -- Exposes :9990 --> E[Mgmt Traffic]
    
    style Control_Plane fill:#f9f,stroke:#333,stroke-width:2px
    style Data_Plane fill:#ccf,stroke:#333,stroke-width:2px
```

### Technical Principles
1.  **Immutability (Docker):** Abandoning "in-place" updates (yum update). Every deployment destroys the previous container and spins up a clean, approved version.
2.  **Idempotency (Ansible):** The playbook can be executed N times without adverse side effects, enforcing the desired state.
3.  **12-Factor App:** Environment-specific configuration (users, passwords, JVM memory) is injected via **Environment Variables**, not by editing `standalone.xml`.

---

## üíª Technical Implementation

<Steps>

1.  **Inventory Definition (IaC)**

    The inventory segregates environments, allowing different policies for `staging` and `production` from a single code base.

    ```ini title="inventory.ini"
    [middleware_nodes]
    # Target node acting as Container Host
    192.168.1.50 ansible_user=dzamo ansible_python_interpreter=/usr/bin/python3
    ```

2.  **Provisioning Playbook**

    This artifact automates the full lifecycle. Note the use of `restart_policy: always` for automatic resilience against daemon failures.

    ```yaml title="deploy-wildfly.yml"
    ---
    - name: Middleware Stack Orchestration (WildFly)
      hosts: middleware_nodes
      become: true
      vars:
        # Strict versioning to avoid accidental upgrades
        wildfly_tag: "24.0.0.Final"
        container_name: "wildfly-prod-core"
        # In production, these would come from Ansible Vault
        mgmt_user: "admin_ops"
        mgmt_pass: "{{ vault_mgmt_pass | default('SecurePass123!') }}"

      tasks:
        - name: Prerequisite Verification (Python SDK)
          apt:
            name: python3-docker
            state: present
            update_cache: yes
            cache_valid_time: 3600

        - name: Pull Approved Base Image
          community.docker.docker_image:
            name: "quay.io/wildfly/wildfly:{{ wildfly_tag }}"
            source: pull

        - name: Container Deployment (Runtime)
          community.docker.docker_container:
            name: "{{ container_name }}"
            image: "quay.io/wildfly/wildfly:{{ wildfly_tag }}"
            state: started
            restart_policy: always
            ports:
              - "8080:8080" # HTTP Traffic
              - "9990:9990" # JBoss CLI Console
            # Binding to 0.0.0.0 is required in containers for external exposure
            command: /opt/jboss/wildfly/bin/standalone.sh -b 0.0.0.0 -bmanagement 0.0.0.0
            env:
              WILDFLY_USERNAME: "{{ mgmt_user }}"
              WILDFLY_PASSWORD: "{{ mgmt_pass }}"
            labels:
              com.company.env: "production"
              com.company.service: "middleware-core"

        - name: Service Health Validation (Healthcheck)
          wait_for:
            host: "127.0.0.1"
            port: 9990
            delay: 5
            timeout: 60
    ```

3.  **Execution & Verification**

    ```bash
    ansible-playbook -i inventory.ini deploy-wildfly.yml
    ```

</Steps>

## üîç Value & Impact Analysis

Implementing this automated flow replaces a manual procedure of **~40 steps** documented in Word/Excel, achieving:

| Metric | Before (Manual) | After (Automated) |
| :--- | :--- | :--- |
| **Provisioning Time** | 4 Hours | < 2 Minutes |
| **Human Error** | High Risk | Eliminated (Idempotent) |
| **Scalability** | Linear (1 admin per N servers) | Exponential |

### Roadmap & Next Steps
*   **Hardening:** Implement vulnerability scanning (Trivy/Clair) in the CI pipeline.
*   **High Availability:** Evolution towards a JBoss Cluster in Domain Mode using **JGroups with TCPPING** for non-multicast environments.

```

---

## Contenido archivo: `src/content/docs/en/projects/middleware/lamp-ansible-docker.mdx`

```bash
$ cat src/content/docs/en/projects/middleware/lamp-ansible-docker.mdx
---
title: "LAMP Stack with Ansible & Docker"
description: Automated deployment of web architecture (Apache, MySQL, PHP) focusing on secret management, custom image builds, and standardized workflows.
sidebar:
  label: LAMP Stack (Web)
  order: 2
type: project
domain: automation
tags: ["ansible", "docker", "lamp", "mysql", "security", "make"]
---

import { Steps } from '@astrojs/starlight/components';

## üìã Executive Summary

This project modernizes the deployment of Legacy PHP applications, evolving from manual configuration to **Immutable Infrastructure**. Unlike basic container deployments, this solution addresses real-world challenges: compiling custom PHP extensions (`mysqli`), managing credentials securely outside the codebase, and standardizing the development environment.

### üéØ Engineering Objectives
1.  **Immutability:** On-demand custom Docker image builds to inject system dependencies during deployment.
2.  **Security (12-Factor App):** Full decoupling of credentials. Source code contains no secrets; they are injected as environment variables.
3.  **Standardization:** Implementation of a `Makefile` wrapper to abstract Python/Ansible complexity and ensure zero-friction onboarding.

---

## üèóÔ∏è Solution Architecture

The workflow implements a remote **Build & Deploy** pattern orchestrated by Ansible. The control node handles configuration and secrets, while the remote host builds and runs the containers.

```mermaid
graph LR
    %% --- Style Definitions ---
    classDef ansibleNode fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000000;
    classDef dockerNode fill:#e0f7fa,stroke:#0288d1,stroke-width:2px,color:#000000;
    classDef secretNode fill:#ffcdd2,stroke:#c62828,stroke-width:2px,color:#000000;

    subgraph Control_Node ["Control Node (Dev)"]
        style Control_Node fill:#eceff1,stroke:#cfd8dc,color:#000000
        Make[("Makefile Wrapper")]:::ansibleNode
        Ansible[("Ansible Playbook")]:::ansibleNode
        Secrets[("secrets.yml<br/>(GitIgnored)")]:::secretNode
        
        Make --> Ansible
        Secrets -.-> Ansible
    end

    subgraph Remote_Host ["App Server (Ubuntu 24.04)"]
        style Remote_Host fill:#0288d1,stroke:#cfd8dc,stroke-width:2px,color:#000000
        
        subgraph Docker_Runtime ["Docker Engine"]
            style Docker_Runtime fill:#eceff1,stroke:#455a64,color:#000000
            
            Build[("Docker Build<br/>(PHP + Mysqli)")]:::dockerNode
            App[("Container Web<br/>Apache")]:::dockerNode
            DB[("Container DB<br/>MariaDB")]:::dockerNode
            
            Ansible -- "1. Upload Dockerfile" --> Build
            Build -- "2. Image Created" --> App
            Ansible -- "3. Inject ENV Vars" --> App
            App -- "Internal DNS" --> DB
        end
    end
    
    linkStyle default stroke:#333,stroke-width:2px,fill:none;
```

---

## üíª Technical Implementation

The solution uses a professional structure separating configuration, secrets, and deployment logic.

<Steps>

1.  **Execution Wrapper (Makefile)**

    To avoid versioning and virtual environment issues, we encapsulate commands in a `Makefile`. This ensures every developer uses the exact same tooling.

    ```makefile title="Makefile"
    setup:
        python3 -m venv .venv
        .venv/bin/pip install ansible-core docker
        .venv/bin/ansible-galaxy install -r requirements.yml

    deploy:
        .venv/bin/ansible-playbook playbook/deploy-lamp.yml
    ```

2.  **Secret Management (Override Pattern)**

    Ansible loads public variables (`all.yml`) first, then overwrites them with a local secret file that is excluded from Git (`.gitignore`).

    ```yaml title="playbook/deploy-lamp.yml"
    - hosts: app_servers
      vars_files:
        - "../group_vars/all.yml"      # Public Structure (Placeholders)
        - "../group_vars/secrets.yml"  # Local Secrets (Real Passwords)
    ```

3.  **Playbook Logic (Build & Run)**

    The playbook not only pulls images but uploads a `Dockerfile` and compiles a custom image to support `mysqli`.

    ```yaml title="playbook/deploy-lamp.yml"
    tasks:
      - name: Build Custom PHP Image
        community.docker.docker_image:
          name: "{{ custom_image_name }}"
          tag: latest
          build:
            path: "{{ remote_project_path }}" # Remote path to Dockerfile
          source: build
          force_source: true

      - name: Deploy Web Container
        community.docker.docker_container:
          name: "{{ project_name }}-web"
          image: "{{ custom_image_name }}:latest" # Use the freshly built image
          env:
            # Secure injection of environment variables
            DB_HOST: "{{ project_name }}-db"
            DB_USER: "{{ mysql_user }}"
            DB_PASSWORD: "{{ mysql_pass }}"
    ```

4.  **Agnostic Code (PHP)**

    The source code (`src/index.php`) contains no credentials. It reads the environment configuration, complying with *The Twelve-Factor App* principles.

    ```php title="src/index.php"
    <?php
    // Secure reading from Environment Variables
    $servername = getenv('DB_HOST');
    $username = getenv('DB_USER');
    $password = getenv('DB_PASSWORD');

    $conn = new mysqli($servername, $username, $password, $dbname);
    
    if ($conn->connect_error) {
        die("‚ùå Connection failed: " . $conn->connect_error);
    }
    echo "‚úÖ Successfully connected to MariaDB.";
    ?>
    ```

5.  **Standardized Execution**

    Thanks to the Makefile, deployment is reduced to two commands, regardless of the underlying complexity.

    ```bash
    # Initialization (First time only)
    make setup

    # Idempotent Deploy
    make deploy
    ```

</Steps>

## üîç Value Analysis

| Feature | Traditional Bash Script | Ansible + Docker Pro |
| :--- | :--- | :--- |
| **Credentials** | Hardcoded in files (`.php`) | Injected in Memory (ENV) |
| **Dependencies** | Manual installation (`apt-get`) | Automatic Docker Build |
| **Portability** | "Works on my machine" | Standardized via Makefile |

### Next Steps (Roadmap AZ-104)

This project serves as the baseline for cloud migration. We have documented this process in the Cloud Operations section:

1. **IaaS Migration:** Replicate this architecture using Azure Virtual Machines.
   - **[üëâ View Scenario 01: LAMP Migration to Azure IaaS](/en/cloud-ops/compute/scenario-01-lamp-iaas/)**

2. **Azure Container Registry (ACR):** Move the build process to the cloud.
3. **Azure Key Vault:** Centralized secret management.
```

---

## Contenido archivo: `./astro.config.mjs`

```bash
$ cat ./astro.config.mjs
// astro.config.mjs - Configuraci√≥n con soporte ingl√©s/espa√±ol
import { defineConfig } from 'astro/config';
import starlight from '@astrojs/starlight';
import mermaid from 'astro-mermaid';

export default defineConfig({
    site: 'https://daniel-zamo.github.io',
    redirects: {
        '/': '/es/',
    },

    integrations: [
        starlight({
            title: 'Daniel Zamo | SysAdmin & DevOps',
            favicon: '/favicon.png',
            social: [
                { icon: 'github', label: 'GitHub', href: 'https://github.com/daniel-zamo/daniel-zamo.github.io' },
                { icon: 'linkedin', label: 'Linkedin', href: 'https://www.linkedin.com/in/danielzamo/' }
            ],

            customCss: ['./src/styles/custom.css'],

            defaultLocale: 'es',
            locales: {
                en: { label: 'English', lang: 'en' },
                es: { label: 'Espa√±ol', lang: 'es' }
            },

            // ... (Secci√≥n head se mantiene igual) ...
            head: [
                {
                    tag: 'script',
                    content: `
                        const storedTheme = localStorage.getItem('starlight-theme');
                        if (!storedTheme) {
                            document.documentElement.setAttribute('data-theme', 'dark');
                            localStorage.setItem('starlight-theme', 'dark');
                        }
                    `,
                },
                // ... (resto de metas) ...
            ],

            // --- ACTUALIZACI√ìN DEL SIDEBAR ---
            sidebar: [
                {
                    label: 'Home',
                    translations: { es: 'Inicio' },
                    link: '/'
                },
                {
                    label: 'Engineering Projects',
                    translations: { es: 'Proyectos de Ingenier√≠a' },
                    collapsed: false,
                    autogenerate: { directory: 'projects' }
                },
                {
                    label: 'Azure Cloud Operations', // <--- NUEVA SECCI√ìN
                    translations: { es: 'Operaciones Cloud Azure' },
                    collapsed: true, // Colapsado para no saturar
                    autogenerate: { directory: 'cloud-ops' }
                },
                {
                    label: 'Linux Systems Engineering',
                    translations: { es: 'Ingenier√≠a de Sistemas Linux' },
                    collapsed: true,
                    autogenerate: { directory: 'linux-ops' }
                },
            ],
        }),
        mermaid(),
    ],
});

```

---

## Contenido archivo: `./package.json`

```bash
$ cat ./package.json
{
  "name": "daniel-zamo.github.io",
  "type": "module",
  "version": "0.0.1",
  "scripts": {
    "dev": "astro dev",
    "start": "astro dev",
    "build": "astro build",
    "preview": "astro preview",
    "astro": "astro"
  },
  "dependencies": {
    "@astrojs/starlight": "^0.37.1",
    "astro": "^5.16.5",
    "astro-mermaid": "^1.1.0",
    "sharp": "^0.34.2"
  }
}

```

---

## Contenido archivo: `./tsconfig.json`

```bash
$ cat ./tsconfig.json
{
  "extends": "astro/tsconfigs/strict",
  "include": [".astro/types.d.ts", "**/*"],
  "exclude": ["dist"]
}

```

---

## Contenido archivo: `./src/content.config.ts`

```bash
$ cat ./src/content.config.ts
import { defineCollection, z } from 'astro:content';
import { docsSchema } from '@astrojs/starlight/schema';

// 1. Colecci√≥n 'docs' (Tu documentaci√≥n principal)
const docsCollection = defineCollection({
  schema: docsSchema({
    extend: z.object({
      // Mapeo de campos personalizados
      source: z.string().optional(),

      // Taxonom√≠a de dominios
      domain: z.enum([
        'compute',
        'storage',
        'networking',
        'security',
        'virtualization',
        'containers',
        'automation',
        'monitoring',
        'cloud'
      ]).optional(),

      // Tipo de contenido (A√±adido 'project' para portfolio)
      type: z.enum([
        'lab',
        'theory',
        'cheatsheet',
        'scenario',
        'configuration',
        'project'
      ]).default('theory'),
    }),
  })
});

// 2. Colecci√≥n 'questions' (Para futuros quizzes)
const questionsCollection = defineCollection({
  loader: async () => [], // Placeholder: Astro 5 pide un loader si no hay archivos f√≠sicos aun
  schema: z.object({
    exam: z.string(),
    topic: z.string(),
    difficulty: z.enum(['easy', 'medium', 'hard']),
    question: z.object({
      scenario: z.string().optional(),
      prompt: z.string(),
    }),
    options: z.array(
      z.object({
        id: z.string(),
        text: z.string(),
      })
    ),
    correctAnswerId: z.string(),
    explanation: z.object({
      summary: z.string(),
      breakdown: z.array(
        z.object({
          optionId: z.string(),
          reasoning: z.string(),
        })
      ),
    }),
  }),
});

// 3. Exportamos
export const collections = {
  docs: docsCollection,
  // Si a√∫n no tienes archivos .json/.yaml en src/content/questions, 
  // puedes comentar la siguiente l√≠nea para evitar warnings de "colecci√≥n vac√≠a"
  questions: questionsCollection,
};


```

---

